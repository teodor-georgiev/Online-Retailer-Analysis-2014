{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from category_encoders import LeaveOneOutEncoder,WOEEncoder\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_theme(palette = \"Dark2\")\n",
    "my_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
    "  (0.8509803921568627, 0.37254901960784315, 0.00784313725490196)]\n",
    "pd.set_option('display.max_columns', None)\n",
    "from itertools import chain, combinations\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from catboost import CatBoostClassifier, Pool, CatBoostRegressor\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "import tensorflow as tensorflow\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_item_id</th>\n",
       "      <th>order_date</th>\n",
       "      <th>delivery_date</th>\n",
       "      <th>item_id</th>\n",
       "      <th>size</th>\n",
       "      <th>item_color</th>\n",
       "      <th>brand_id</th>\n",
       "      <th>item_price</th>\n",
       "      <th>user_id</th>\n",
       "      <th>user_title</th>\n",
       "      <th>user_dob</th>\n",
       "      <th>user_state</th>\n",
       "      <th>user_reg_date</th>\n",
       "      <th>return</th>\n",
       "      <th>delivery_time</th>\n",
       "      <th>order_id</th>\n",
       "      <th>user_age</th>\n",
       "      <th>user_reg_age</th>\n",
       "      <th>order_weekday</th>\n",
       "      <th>delivery_weekday</th>\n",
       "      <th>order_month</th>\n",
       "      <th>delivery_month</th>\n",
       "      <th>order_day</th>\n",
       "      <th>delivery_day</th>\n",
       "      <th>order_week</th>\n",
       "      <th>delivery_week</th>\n",
       "      <th>order_item_count</th>\n",
       "      <th>order_sum</th>\n",
       "      <th>average_item_price_order</th>\n",
       "      <th>order_number_same_item_id</th>\n",
       "      <th>order_number_different_item_id</th>\n",
       "      <th>order_number_same_size</th>\n",
       "      <th>order_number_different_size</th>\n",
       "      <th>order_number_same_item_color</th>\n",
       "      <th>order_number_different_item_color</th>\n",
       "      <th>order_number_same_brand_id</th>\n",
       "      <th>order_number_different_brand_id</th>\n",
       "      <th>order_number_same_item_id_size</th>\n",
       "      <th>order_number_different_item_id_size</th>\n",
       "      <th>order_number_same_item_id_item_color</th>\n",
       "      <th>order_number_different_item_id_item_color</th>\n",
       "      <th>order_number_same_size_item_color</th>\n",
       "      <th>order_number_different_size_item_color</th>\n",
       "      <th>order_number_same_size_brand_id</th>\n",
       "      <th>order_number_different_size_brand_id</th>\n",
       "      <th>order_number_same_item_color_brand_id</th>\n",
       "      <th>order_number_different_item_color_brand_id</th>\n",
       "      <th>order_number_same_item_id_size_item_color</th>\n",
       "      <th>order_number_different_item_id_size_item_color</th>\n",
       "      <th>order_number_same_size_item_color_brand_id</th>\n",
       "      <th>order_number_different_size_item_color_brand_id</th>\n",
       "      <th>order_item_id_nunique</th>\n",
       "      <th>order_size_nunique</th>\n",
       "      <th>order_brand_id_nunique</th>\n",
       "      <th>order_item_color_nunique</th>\n",
       "      <th>order_item_id_color_nunique</th>\n",
       "      <th>order_item_id_size_nunique</th>\n",
       "      <th>order_brand_id_color_nunique</th>\n",
       "      <th>order_brand_id_size_nunique</th>\n",
       "      <th>order_brand_id_item_id_nunique</th>\n",
       "      <th>item_price_item_id_mean</th>\n",
       "      <th>item_price_item_id_std</th>\n",
       "      <th>item_price_item_id_min</th>\n",
       "      <th>item_price_item_id_max</th>\n",
       "      <th>item_price_item_id_sum</th>\n",
       "      <th>item_price_item_id_count</th>\n",
       "      <th>item_price_item_id_median</th>\n",
       "      <th>item_price_item_id_mad</th>\n",
       "      <th>item_price_user_id_mean</th>\n",
       "      <th>item_price_user_id_std</th>\n",
       "      <th>item_price_user_id_min</th>\n",
       "      <th>item_price_user_id_max</th>\n",
       "      <th>item_price_user_id_sum</th>\n",
       "      <th>item_price_user_id_count</th>\n",
       "      <th>item_price_user_id_median</th>\n",
       "      <th>item_price_user_id_mad</th>\n",
       "      <th>item_price_brand_id_mean</th>\n",
       "      <th>item_price_brand_id_std</th>\n",
       "      <th>item_price_brand_id_min</th>\n",
       "      <th>item_price_brand_id_max</th>\n",
       "      <th>item_price_brand_id_sum</th>\n",
       "      <th>item_price_brand_id_count</th>\n",
       "      <th>item_price_brand_id_median</th>\n",
       "      <th>item_price_brand_id_mad</th>\n",
       "      <th>mode_item_id</th>\n",
       "      <th>mode_size</th>\n",
       "      <th>mode_brand_id</th>\n",
       "      <th>mode_item_color</th>\n",
       "      <th>price-item_price_item_id_mean</th>\n",
       "      <th>price-item_price_item_id_min</th>\n",
       "      <th>price-item_price_item_id_max</th>\n",
       "      <th>price-item_price_user_id_mean</th>\n",
       "      <th>price-item_price_user_id_min</th>\n",
       "      <th>price-item_price_user_id_max</th>\n",
       "      <th>price-item_price_brand_id_mean</th>\n",
       "      <th>price-item_price_brand_id_min</th>\n",
       "      <th>price-item_price_brand_id_max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>186</td>\n",
       "      <td>s</td>\n",
       "      <td>denim</td>\n",
       "      <td>25</td>\n",
       "      <td>69.90</td>\n",
       "      <td>794</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1965-01-06</td>\n",
       "      <td>Bad-Wue</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-01_794</td>\n",
       "      <td>47</td>\n",
       "      <td>342</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>209.8</td>\n",
       "      <td>69.93</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>69.47</td>\n",
       "      <td>3.41</td>\n",
       "      <td>39.90</td>\n",
       "      <td>69.90</td>\n",
       "      <td>21606.68</td>\n",
       "      <td>311</td>\n",
       "      <td>69.90</td>\n",
       "      <td>0.83</td>\n",
       "      <td>62.65</td>\n",
       "      <td>24.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.9</td>\n",
       "      <td>689.1</td>\n",
       "      <td>11</td>\n",
       "      <td>69.9</td>\n",
       "      <td>16.52</td>\n",
       "      <td>55.23</td>\n",
       "      <td>13.65</td>\n",
       "      <td>24.9</td>\n",
       "      <td>79.90</td>\n",
       "      <td>282536.19</td>\n",
       "      <td>5116</td>\n",
       "      <td>59.90</td>\n",
       "      <td>11.66</td>\n",
       "      <td>71</td>\n",
       "      <td>s</td>\n",
       "      <td>20</td>\n",
       "      <td>white</td>\n",
       "      <td>-0.43</td>\n",
       "      <td>-30.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.25</td>\n",
       "      <td>-69.90</td>\n",
       "      <td>20.00</td>\n",
       "      <td>-14.67</td>\n",
       "      <td>-45.00</td>\n",
       "      <td>10.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>71</td>\n",
       "      <td>unsized</td>\n",
       "      <td>ocher</td>\n",
       "      <td>21</td>\n",
       "      <td>69.95</td>\n",
       "      <td>794</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1965-01-06</td>\n",
       "      <td>Bad-Wue</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-01_794</td>\n",
       "      <td>47</td>\n",
       "      <td>342</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>209.8</td>\n",
       "      <td>69.93</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.95</td>\n",
       "      <td>13.28</td>\n",
       "      <td>34.95</td>\n",
       "      <td>69.95</td>\n",
       "      <td>59108.70</td>\n",
       "      <td>1020</td>\n",
       "      <td>69.95</td>\n",
       "      <td>12.35</td>\n",
       "      <td>62.65</td>\n",
       "      <td>24.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.9</td>\n",
       "      <td>689.1</td>\n",
       "      <td>11</td>\n",
       "      <td>69.9</td>\n",
       "      <td>16.52</td>\n",
       "      <td>74.54</td>\n",
       "      <td>30.32</td>\n",
       "      <td>24.9</td>\n",
       "      <td>179.95</td>\n",
       "      <td>478991.63</td>\n",
       "      <td>6426</td>\n",
       "      <td>69.95</td>\n",
       "      <td>22.66</td>\n",
       "      <td>71</td>\n",
       "      <td>s</td>\n",
       "      <td>20</td>\n",
       "      <td>white</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.30</td>\n",
       "      <td>-69.95</td>\n",
       "      <td>19.95</td>\n",
       "      <td>4.59</td>\n",
       "      <td>-45.05</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>2012-04-01</td>\n",
       "      <td>2012-04-03</td>\n",
       "      <td>71</td>\n",
       "      <td>unsized</td>\n",
       "      <td>curry</td>\n",
       "      <td>21</td>\n",
       "      <td>69.95</td>\n",
       "      <td>794</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1965-01-06</td>\n",
       "      <td>Bad-Wue</td>\n",
       "      <td>2011-04-25</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2012-04-01_794</td>\n",
       "      <td>47</td>\n",
       "      <td>342</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>209.8</td>\n",
       "      <td>69.93</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>57.95</td>\n",
       "      <td>13.28</td>\n",
       "      <td>34.95</td>\n",
       "      <td>69.95</td>\n",
       "      <td>59108.70</td>\n",
       "      <td>1020</td>\n",
       "      <td>69.95</td>\n",
       "      <td>12.35</td>\n",
       "      <td>62.65</td>\n",
       "      <td>24.51</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.9</td>\n",
       "      <td>689.1</td>\n",
       "      <td>11</td>\n",
       "      <td>69.9</td>\n",
       "      <td>16.52</td>\n",
       "      <td>74.54</td>\n",
       "      <td>30.32</td>\n",
       "      <td>24.9</td>\n",
       "      <td>179.95</td>\n",
       "      <td>478991.63</td>\n",
       "      <td>6426</td>\n",
       "      <td>69.95</td>\n",
       "      <td>22.66</td>\n",
       "      <td>71</td>\n",
       "      <td>s</td>\n",
       "      <td>20</td>\n",
       "      <td>white</td>\n",
       "      <td>-12.00</td>\n",
       "      <td>-35.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-7.30</td>\n",
       "      <td>-69.95</td>\n",
       "      <td>19.95</td>\n",
       "      <td>4.59</td>\n",
       "      <td>-45.05</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-02</td>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>22</td>\n",
       "      <td>s</td>\n",
       "      <td>green</td>\n",
       "      <td>14</td>\n",
       "      <td>39.90</td>\n",
       "      <td>808</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1959-11-09</td>\n",
       "      <td>Saxony</td>\n",
       "      <td>2012-01-04</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-02_808</td>\n",
       "      <td>52</td>\n",
       "      <td>89</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>31.10</td>\n",
       "      <td>8.68</td>\n",
       "      <td>19.90</td>\n",
       "      <td>39.90</td>\n",
       "      <td>68669.20</td>\n",
       "      <td>2208</td>\n",
       "      <td>29.90</td>\n",
       "      <td>8.41</td>\n",
       "      <td>39.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td>39.9</td>\n",
       "      <td>1</td>\n",
       "      <td>39.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>55.16</td>\n",
       "      <td>19.34</td>\n",
       "      <td>0.0</td>\n",
       "      <td>89.90</td>\n",
       "      <td>629751.00</td>\n",
       "      <td>11417</td>\n",
       "      <td>49.90</td>\n",
       "      <td>16.21</td>\n",
       "      <td>22</td>\n",
       "      <td>s</td>\n",
       "      <td>14</td>\n",
       "      <td>green</td>\n",
       "      <td>-8.80</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>15.26</td>\n",
       "      <td>-39.90</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>2012-04-02</td>\n",
       "      <td>2012-04-06</td>\n",
       "      <td>151</td>\n",
       "      <td>s</td>\n",
       "      <td>black</td>\n",
       "      <td>53</td>\n",
       "      <td>29.90</td>\n",
       "      <td>825</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1964-07-11</td>\n",
       "      <td>S-Holstein</td>\n",
       "      <td>2011-02-16</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2012-04-02_825</td>\n",
       "      <td>48</td>\n",
       "      <td>411</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>3</td>\n",
       "      <td>249.7</td>\n",
       "      <td>83.23</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>27.59</td>\n",
       "      <td>4.02</td>\n",
       "      <td>24.90</td>\n",
       "      <td>49.90</td>\n",
       "      <td>1434.80</td>\n",
       "      <td>52</td>\n",
       "      <td>24.90</td>\n",
       "      <td>2.90</td>\n",
       "      <td>79.34</td>\n",
       "      <td>45.03</td>\n",
       "      <td>29.9</td>\n",
       "      <td>139.9</td>\n",
       "      <td>714.1</td>\n",
       "      <td>9</td>\n",
       "      <td>69.9</td>\n",
       "      <td>38.27</td>\n",
       "      <td>41.11</td>\n",
       "      <td>13.08</td>\n",
       "      <td>19.9</td>\n",
       "      <td>79.90</td>\n",
       "      <td>99614.71</td>\n",
       "      <td>2423</td>\n",
       "      <td>39.90</td>\n",
       "      <td>10.10</td>\n",
       "      <td>15</td>\n",
       "      <td>xl</td>\n",
       "      <td>37</td>\n",
       "      <td>black</td>\n",
       "      <td>-2.31</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>20.0</td>\n",
       "      <td>49.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>110.00</td>\n",
       "      <td>11.21</td>\n",
       "      <td>-10.00</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531165</th>\n",
       "      <td>50074</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>2342</td>\n",
       "      <td>s</td>\n",
       "      <td>terracotta</td>\n",
       "      <td>5</td>\n",
       "      <td>69.90</td>\n",
       "      <td>91920</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1962-03-08</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-29_91920</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>748.2</td>\n",
       "      <td>83.13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>62.24</td>\n",
       "      <td>4.26</td>\n",
       "      <td>49.90</td>\n",
       "      <td>69.90</td>\n",
       "      <td>106988.10</td>\n",
       "      <td>1719</td>\n",
       "      <td>59.90</td>\n",
       "      <td>3.60</td>\n",
       "      <td>83.13</td>\n",
       "      <td>20.82</td>\n",
       "      <td>64.9</td>\n",
       "      <td>129.0</td>\n",
       "      <td>748.2</td>\n",
       "      <td>9</td>\n",
       "      <td>79.9</td>\n",
       "      <td>15.42</td>\n",
       "      <td>64.66</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>2941212.79</td>\n",
       "      <td>45487</td>\n",
       "      <td>69.90</td>\n",
       "      <td>15.02</td>\n",
       "      <td>2505</td>\n",
       "      <td>s</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "      <td>-7.66</td>\n",
       "      <td>-20.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>13.23</td>\n",
       "      <td>-5.00</td>\n",
       "      <td>59.10</td>\n",
       "      <td>-5.24</td>\n",
       "      <td>-69.90</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531166</th>\n",
       "      <td>50075</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>2013-05-03</td>\n",
       "      <td>2505</td>\n",
       "      <td>s</td>\n",
       "      <td>terracotta</td>\n",
       "      <td>5</td>\n",
       "      <td>64.90</td>\n",
       "      <td>91920</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1962-03-08</td>\n",
       "      <td>Bayern</td>\n",
       "      <td>2013-04-29</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-29_91920</td>\n",
       "      <td>51</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>29</td>\n",
       "      <td>3</td>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>9</td>\n",
       "      <td>748.2</td>\n",
       "      <td>83.13</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>66.79</td>\n",
       "      <td>2.67</td>\n",
       "      <td>59.90</td>\n",
       "      <td>69.90</td>\n",
       "      <td>26650.10</td>\n",
       "      <td>399</td>\n",
       "      <td>64.90</td>\n",
       "      <td>2.51</td>\n",
       "      <td>83.13</td>\n",
       "      <td>20.82</td>\n",
       "      <td>64.9</td>\n",
       "      <td>129.0</td>\n",
       "      <td>748.2</td>\n",
       "      <td>9</td>\n",
       "      <td>79.9</td>\n",
       "      <td>15.42</td>\n",
       "      <td>64.66</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>2941212.79</td>\n",
       "      <td>45487</td>\n",
       "      <td>69.90</td>\n",
       "      <td>15.02</td>\n",
       "      <td>2505</td>\n",
       "      <td>s</td>\n",
       "      <td>5</td>\n",
       "      <td>red</td>\n",
       "      <td>1.89</td>\n",
       "      <td>-5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>18.23</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.10</td>\n",
       "      <td>-0.24</td>\n",
       "      <td>-64.90</td>\n",
       "      <td>105.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531167</th>\n",
       "      <td>50076</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>2470</td>\n",
       "      <td>l</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>79.90</td>\n",
       "      <td>85095</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1950-02-14</td>\n",
       "      <td>Berlin</td>\n",
       "      <td>2013-03-24</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-28_85095</td>\n",
       "      <td>63</td>\n",
       "      <td>35</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>79.9</td>\n",
       "      <td>79.90</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>79.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.90</td>\n",
       "      <td>79.90</td>\n",
       "      <td>112259.50</td>\n",
       "      <td>1405</td>\n",
       "      <td>79.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>79.9</td>\n",
       "      <td>79.9</td>\n",
       "      <td>239.7</td>\n",
       "      <td>3</td>\n",
       "      <td>79.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.66</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>2941212.79</td>\n",
       "      <td>45487</td>\n",
       "      <td>69.90</td>\n",
       "      <td>15.02</td>\n",
       "      <td>2470</td>\n",
       "      <td>l</td>\n",
       "      <td>5</td>\n",
       "      <td>ocher</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>-15.24</td>\n",
       "      <td>-79.90</td>\n",
       "      <td>90.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531168</th>\n",
       "      <td>50077</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>2452</td>\n",
       "      <td>m</td>\n",
       "      <td>white</td>\n",
       "      <td>5</td>\n",
       "      <td>59.90</td>\n",
       "      <td>91922</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1969-11-27</td>\n",
       "      <td>Bburg</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-28_91922</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>119.8</td>\n",
       "      <td>59.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.90</td>\n",
       "      <td>59.90</td>\n",
       "      <td>68166.20</td>\n",
       "      <td>1138</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.9</td>\n",
       "      <td>59.9</td>\n",
       "      <td>119.8</td>\n",
       "      <td>2</td>\n",
       "      <td>59.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.66</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>2941212.79</td>\n",
       "      <td>45487</td>\n",
       "      <td>69.90</td>\n",
       "      <td>15.02</td>\n",
       "      <td>2452</td>\n",
       "      <td>m</td>\n",
       "      <td>5</td>\n",
       "      <td>black</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.76</td>\n",
       "      <td>-59.90</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>531169</th>\n",
       "      <td>50078</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>2013-05-02</td>\n",
       "      <td>2452</td>\n",
       "      <td>m</td>\n",
       "      <td>black</td>\n",
       "      <td>5</td>\n",
       "      <td>59.90</td>\n",
       "      <td>91922</td>\n",
       "      <td>Mrs</td>\n",
       "      <td>1969-11-27</td>\n",
       "      <td>Bburg</td>\n",
       "      <td>2013-04-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>2013-04-28_91922</td>\n",
       "      <td>43</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>28</td>\n",
       "      <td>2</td>\n",
       "      <td>17</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>119.8</td>\n",
       "      <td>59.90</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.90</td>\n",
       "      <td>59.90</td>\n",
       "      <td>68166.20</td>\n",
       "      <td>1138</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.90</td>\n",
       "      <td>0.00</td>\n",
       "      <td>59.9</td>\n",
       "      <td>59.9</td>\n",
       "      <td>119.8</td>\n",
       "      <td>2</td>\n",
       "      <td>59.9</td>\n",
       "      <td>0.00</td>\n",
       "      <td>64.66</td>\n",
       "      <td>20.40</td>\n",
       "      <td>0.0</td>\n",
       "      <td>169.90</td>\n",
       "      <td>2941212.79</td>\n",
       "      <td>45487</td>\n",
       "      <td>69.90</td>\n",
       "      <td>15.02</td>\n",
       "      <td>2452</td>\n",
       "      <td>m</td>\n",
       "      <td>5</td>\n",
       "      <td>black</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>4.76</td>\n",
       "      <td>-59.90</td>\n",
       "      <td>110.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>531170 rows × 97 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        order_item_id  order_date delivery_date  item_id     size  item_color  \\\n",
       "0                   1  2012-04-01    2012-04-03      186        s       denim   \n",
       "1                   2  2012-04-01    2012-04-03       71  unsized       ocher   \n",
       "2                   3  2012-04-01    2012-04-03       71  unsized       curry   \n",
       "3                   4  2012-04-02    2012-04-06       22        s       green   \n",
       "4                   5  2012-04-02    2012-04-06      151        s       black   \n",
       "...               ...         ...           ...      ...      ...         ...   \n",
       "531165          50074  2013-04-29    2013-05-03     2342        s  terracotta   \n",
       "531166          50075  2013-04-29    2013-05-03     2505        s  terracotta   \n",
       "531167          50076  2013-04-28    2013-05-02     2470        l       white   \n",
       "531168          50077  2013-04-28    2013-05-02     2452        m       white   \n",
       "531169          50078  2013-04-28    2013-05-02     2452        m       black   \n",
       "\n",
       "        brand_id  item_price  user_id user_title    user_dob  user_state  \\\n",
       "0             25       69.90      794        Mrs  1965-01-06     Bad-Wue   \n",
       "1             21       69.95      794        Mrs  1965-01-06     Bad-Wue   \n",
       "2             21       69.95      794        Mrs  1965-01-06     Bad-Wue   \n",
       "3             14       39.90      808        Mrs  1959-11-09      Saxony   \n",
       "4             53       29.90      825        Mrs  1964-07-11  S-Holstein   \n",
       "...          ...         ...      ...        ...         ...         ...   \n",
       "531165         5       69.90    91920        Mrs  1962-03-08      Bayern   \n",
       "531166         5       64.90    91920        Mrs  1962-03-08      Bayern   \n",
       "531167         5       79.90    85095        Mrs  1950-02-14      Berlin   \n",
       "531168         5       59.90    91922        Mrs  1969-11-27       Bburg   \n",
       "531169         5       59.90    91922        Mrs  1969-11-27       Bburg   \n",
       "\n",
       "       user_reg_date  return  delivery_time          order_id  user_age  \\\n",
       "0         2011-04-25     0.0              2    2012-04-01_794        47   \n",
       "1         2011-04-25     1.0              2    2012-04-01_794        47   \n",
       "2         2011-04-25     1.0              2    2012-04-01_794        47   \n",
       "3         2012-01-04     0.0              4    2012-04-02_808        52   \n",
       "4         2011-02-16     0.0              4    2012-04-02_825        48   \n",
       "...              ...     ...            ...               ...       ...   \n",
       "531165    2013-04-29     NaN              4  2013-04-29_91920        51   \n",
       "531166    2013-04-29     NaN              4  2013-04-29_91920        51   \n",
       "531167    2013-03-24     NaN              4  2013-04-28_85095        63   \n",
       "531168    2013-04-28     NaN              4  2013-04-28_91922        43   \n",
       "531169    2013-04-28     NaN              4  2013-04-28_91922        43   \n",
       "\n",
       "        user_reg_age  order_weekday  delivery_weekday  order_month  \\\n",
       "0                342              6                 1            4   \n",
       "1                342              6                 1            4   \n",
       "2                342              6                 1            4   \n",
       "3                 89              0                 4            4   \n",
       "4                411              0                 4            4   \n",
       "...              ...            ...               ...          ...   \n",
       "531165             0              0                 4            4   \n",
       "531166             0              0                 4            4   \n",
       "531167            35              6                 3            4   \n",
       "531168             0              6                 3            4   \n",
       "531169             0              6                 3            4   \n",
       "\n",
       "        delivery_month  order_day  delivery_day  order_week  delivery_week  \\\n",
       "0                    4          1             3          13             14   \n",
       "1                    4          1             3          13             14   \n",
       "2                    4          1             3          13             14   \n",
       "3                    4          2             6          14             14   \n",
       "4                    4          2             6          14             14   \n",
       "...                ...        ...           ...         ...            ...   \n",
       "531165               5         29             3          18             18   \n",
       "531166               5         29             3          18             18   \n",
       "531167               5         28             2          17             18   \n",
       "531168               5         28             2          17             18   \n",
       "531169               5         28             2          17             18   \n",
       "\n",
       "        order_item_count  order_sum  average_item_price_order  \\\n",
       "0                      3      209.8                     69.93   \n",
       "1                      3      209.8                     69.93   \n",
       "2                      3      209.8                     69.93   \n",
       "3                      1       39.9                     39.90   \n",
       "4                      3      249.7                     83.23   \n",
       "...                  ...        ...                       ...   \n",
       "531165                 9      748.2                     83.13   \n",
       "531166                 9      748.2                     83.13   \n",
       "531167                 1       79.9                     79.90   \n",
       "531168                 2      119.8                     59.90   \n",
       "531169                 2      119.8                     59.90   \n",
       "\n",
       "        order_number_same_item_id  order_number_different_item_id  \\\n",
       "0                               1                               2   \n",
       "1                               2                               1   \n",
       "2                               2                               1   \n",
       "3                               1                               0   \n",
       "4                               1                               2   \n",
       "...                           ...                             ...   \n",
       "531165                          1                               8   \n",
       "531166                          2                               7   \n",
       "531167                          1                               0   \n",
       "531168                          2                               0   \n",
       "531169                          2                               0   \n",
       "\n",
       "        order_number_same_size  order_number_different_size  \\\n",
       "0                            1                            2   \n",
       "1                            2                            1   \n",
       "2                            2                            1   \n",
       "3                            1                            0   \n",
       "4                            2                            1   \n",
       "...                        ...                          ...   \n",
       "531165                       5                            4   \n",
       "531166                       5                            4   \n",
       "531167                       1                            0   \n",
       "531168                       2                            0   \n",
       "531169                       2                            0   \n",
       "\n",
       "        order_number_same_item_color  order_number_different_item_color  \\\n",
       "0                                  1                                  2   \n",
       "1                                  1                                  2   \n",
       "2                                  1                                  2   \n",
       "3                                  1                                  0   \n",
       "4                                  2                                  1   \n",
       "...                              ...                                ...   \n",
       "531165                             3                                  6   \n",
       "531166                             3                                  6   \n",
       "531167                             1                                  0   \n",
       "531168                             1                                  1   \n",
       "531169                             1                                  1   \n",
       "\n",
       "        order_number_same_brand_id  order_number_different_brand_id  \\\n",
       "0                                1                                2   \n",
       "1                                2                                1   \n",
       "2                                2                                1   \n",
       "3                                1                                0   \n",
       "4                                1                                2   \n",
       "...                            ...                              ...   \n",
       "531165                           4                                5   \n",
       "531166                           4                                5   \n",
       "531167                           1                                0   \n",
       "531168                           2                                0   \n",
       "531169                           2                                0   \n",
       "\n",
       "        order_number_same_item_id_size  order_number_different_item_id_size  \\\n",
       "0                                    1                                    2   \n",
       "1                                    2                                    1   \n",
       "2                                    2                                    1   \n",
       "3                                    1                                    0   \n",
       "4                                    1                                    2   \n",
       "...                                ...                                  ...   \n",
       "531165                               1                                    8   \n",
       "531166                               2                                    7   \n",
       "531167                               1                                    0   \n",
       "531168                               2                                    0   \n",
       "531169                               2                                    0   \n",
       "\n",
       "        order_number_same_item_id_item_color  \\\n",
       "0                                          1   \n",
       "1                                          1   \n",
       "2                                          1   \n",
       "3                                          1   \n",
       "4                                          1   \n",
       "...                                      ...   \n",
       "531165                                     1   \n",
       "531166                                     2   \n",
       "531167                                     1   \n",
       "531168                                     1   \n",
       "531169                                     1   \n",
       "\n",
       "        order_number_different_item_id_item_color  \\\n",
       "0                                               2   \n",
       "1                                               2   \n",
       "2                                               2   \n",
       "3                                               0   \n",
       "4                                               2   \n",
       "...                                           ...   \n",
       "531165                                          8   \n",
       "531166                                          7   \n",
       "531167                                          0   \n",
       "531168                                          1   \n",
       "531169                                          1   \n",
       "\n",
       "        order_number_same_size_item_color  \\\n",
       "0                                       1   \n",
       "1                                       1   \n",
       "2                                       1   \n",
       "3                                       1   \n",
       "4                                       2   \n",
       "...                                   ...   \n",
       "531165                                  3   \n",
       "531166                                  3   \n",
       "531167                                  1   \n",
       "531168                                  1   \n",
       "531169                                  1   \n",
       "\n",
       "        order_number_different_size_item_color  \\\n",
       "0                                            2   \n",
       "1                                            2   \n",
       "2                                            2   \n",
       "3                                            0   \n",
       "4                                            1   \n",
       "...                                        ...   \n",
       "531165                                       6   \n",
       "531166                                       6   \n",
       "531167                                       0   \n",
       "531168                                       1   \n",
       "531169                                       1   \n",
       "\n",
       "        order_number_same_size_brand_id  order_number_different_size_brand_id  \\\n",
       "0                                     1                                     2   \n",
       "1                                     2                                     1   \n",
       "2                                     2                                     1   \n",
       "3                                     1                                     0   \n",
       "4                                     1                                     2   \n",
       "...                                 ...                                   ...   \n",
       "531165                                4                                     5   \n",
       "531166                                4                                     5   \n",
       "531167                                1                                     0   \n",
       "531168                                2                                     0   \n",
       "531169                                2                                     0   \n",
       "\n",
       "        order_number_same_item_color_brand_id  \\\n",
       "0                                           1   \n",
       "1                                           1   \n",
       "2                                           1   \n",
       "3                                           1   \n",
       "4                                           1   \n",
       "...                                       ...   \n",
       "531165                                      3   \n",
       "531166                                      3   \n",
       "531167                                      1   \n",
       "531168                                      1   \n",
       "531169                                      1   \n",
       "\n",
       "        order_number_different_item_color_brand_id  \\\n",
       "0                                                2   \n",
       "1                                                2   \n",
       "2                                                2   \n",
       "3                                                0   \n",
       "4                                                2   \n",
       "...                                            ...   \n",
       "531165                                           6   \n",
       "531166                                           6   \n",
       "531167                                           0   \n",
       "531168                                           1   \n",
       "531169                                           1   \n",
       "\n",
       "        order_number_same_item_id_size_item_color  \\\n",
       "0                                               1   \n",
       "1                                               1   \n",
       "2                                               1   \n",
       "3                                               1   \n",
       "4                                               1   \n",
       "...                                           ...   \n",
       "531165                                          1   \n",
       "531166                                          2   \n",
       "531167                                          1   \n",
       "531168                                          1   \n",
       "531169                                          1   \n",
       "\n",
       "        order_number_different_item_id_size_item_color  \\\n",
       "0                                                    2   \n",
       "1                                                    2   \n",
       "2                                                    2   \n",
       "3                                                    0   \n",
       "4                                                    2   \n",
       "...                                                ...   \n",
       "531165                                               8   \n",
       "531166                                               7   \n",
       "531167                                               0   \n",
       "531168                                               1   \n",
       "531169                                               1   \n",
       "\n",
       "        order_number_same_size_item_color_brand_id  \\\n",
       "0                                                1   \n",
       "1                                                1   \n",
       "2                                                1   \n",
       "3                                                1   \n",
       "4                                                1   \n",
       "...                                            ...   \n",
       "531165                                           3   \n",
       "531166                                           3   \n",
       "531167                                           1   \n",
       "531168                                           1   \n",
       "531169                                           1   \n",
       "\n",
       "        order_number_different_size_item_color_brand_id  \\\n",
       "0                                                     2   \n",
       "1                                                     2   \n",
       "2                                                     2   \n",
       "3                                                     0   \n",
       "4                                                     2   \n",
       "...                                                 ...   \n",
       "531165                                                6   \n",
       "531166                                                6   \n",
       "531167                                                0   \n",
       "531168                                                1   \n",
       "531169                                                1   \n",
       "\n",
       "        order_item_id_nunique  order_size_nunique  order_brand_id_nunique  \\\n",
       "0                           2                   2                       2   \n",
       "1                           2                   2                       2   \n",
       "2                           2                   2                       2   \n",
       "3                           1                   1                       1   \n",
       "4                           3                   2                       3   \n",
       "...                       ...                 ...                     ...   \n",
       "531165                      8                   2                       6   \n",
       "531166                      8                   2                       6   \n",
       "531167                      1                   1                       1   \n",
       "531168                      1                   1                       1   \n",
       "531169                      1                   1                       1   \n",
       "\n",
       "        order_item_color_nunique  order_item_id_color_nunique  \\\n",
       "0                              3                            1   \n",
       "1                              3                            2   \n",
       "2                              3                            2   \n",
       "3                              1                            1   \n",
       "4                              2                            1   \n",
       "...                          ...                          ...   \n",
       "531165                         5                            1   \n",
       "531166                         5                            1   \n",
       "531167                         1                            1   \n",
       "531168                         2                            2   \n",
       "531169                         2                            2   \n",
       "\n",
       "        order_item_id_size_nunique  order_brand_id_color_nunique  \\\n",
       "0                                1                             1   \n",
       "1                                1                             2   \n",
       "2                                1                             2   \n",
       "3                                1                             1   \n",
       "4                                1                             1   \n",
       "...                            ...                           ...   \n",
       "531165                           1                             2   \n",
       "531166                           1                             2   \n",
       "531167                           1                             1   \n",
       "531168                           1                             2   \n",
       "531169                           1                             2   \n",
       "\n",
       "        order_brand_id_size_nunique  order_brand_id_item_id_nunique  \\\n",
       "0                                 1                               1   \n",
       "1                                 1                               1   \n",
       "2                                 1                               1   \n",
       "3                                 1                               1   \n",
       "4                                 1                               1   \n",
       "...                             ...                             ...   \n",
       "531165                            1                               3   \n",
       "531166                            1                               3   \n",
       "531167                            1                               1   \n",
       "531168                            1                               1   \n",
       "531169                            1                               1   \n",
       "\n",
       "        item_price_item_id_mean  item_price_item_id_std  \\\n",
       "0                         69.47                    3.41   \n",
       "1                         57.95                   13.28   \n",
       "2                         57.95                   13.28   \n",
       "3                         31.10                    8.68   \n",
       "4                         27.59                    4.02   \n",
       "...                         ...                     ...   \n",
       "531165                    62.24                    4.26   \n",
       "531166                    66.79                    2.67   \n",
       "531167                    79.90                    0.00   \n",
       "531168                    59.90                    0.00   \n",
       "531169                    59.90                    0.00   \n",
       "\n",
       "        item_price_item_id_min  item_price_item_id_max  \\\n",
       "0                        39.90                   69.90   \n",
       "1                        34.95                   69.95   \n",
       "2                        34.95                   69.95   \n",
       "3                        19.90                   39.90   \n",
       "4                        24.90                   49.90   \n",
       "...                        ...                     ...   \n",
       "531165                   49.90                   69.90   \n",
       "531166                   59.90                   69.90   \n",
       "531167                   79.90                   79.90   \n",
       "531168                   59.90                   59.90   \n",
       "531169                   59.90                   59.90   \n",
       "\n",
       "        item_price_item_id_sum  item_price_item_id_count  \\\n",
       "0                     21606.68                       311   \n",
       "1                     59108.70                      1020   \n",
       "2                     59108.70                      1020   \n",
       "3                     68669.20                      2208   \n",
       "4                      1434.80                        52   \n",
       "...                        ...                       ...   \n",
       "531165               106988.10                      1719   \n",
       "531166                26650.10                       399   \n",
       "531167               112259.50                      1405   \n",
       "531168                68166.20                      1138   \n",
       "531169                68166.20                      1138   \n",
       "\n",
       "        item_price_item_id_median  item_price_item_id_mad  \\\n",
       "0                           69.90                    0.83   \n",
       "1                           69.95                   12.35   \n",
       "2                           69.95                   12.35   \n",
       "3                           29.90                    8.41   \n",
       "4                           24.90                    2.90   \n",
       "...                           ...                     ...   \n",
       "531165                      59.90                    3.60   \n",
       "531166                      64.90                    2.51   \n",
       "531167                      79.90                    0.00   \n",
       "531168                      59.90                    0.00   \n",
       "531169                      59.90                    0.00   \n",
       "\n",
       "        item_price_user_id_mean  item_price_user_id_std  \\\n",
       "0                         62.65                   24.51   \n",
       "1                         62.65                   24.51   \n",
       "2                         62.65                   24.51   \n",
       "3                         39.90                    0.00   \n",
       "4                         79.34                   45.03   \n",
       "...                         ...                     ...   \n",
       "531165                    83.13                   20.82   \n",
       "531166                    83.13                   20.82   \n",
       "531167                    79.90                    0.00   \n",
       "531168                    59.90                    0.00   \n",
       "531169                    59.90                    0.00   \n",
       "\n",
       "        item_price_user_id_min  item_price_user_id_max  \\\n",
       "0                          0.0                    89.9   \n",
       "1                          0.0                    89.9   \n",
       "2                          0.0                    89.9   \n",
       "3                         39.9                    39.9   \n",
       "4                         29.9                   139.9   \n",
       "...                        ...                     ...   \n",
       "531165                    64.9                   129.0   \n",
       "531166                    64.9                   129.0   \n",
       "531167                    79.9                    79.9   \n",
       "531168                    59.9                    59.9   \n",
       "531169                    59.9                    59.9   \n",
       "\n",
       "        item_price_user_id_sum  item_price_user_id_count  \\\n",
       "0                        689.1                        11   \n",
       "1                        689.1                        11   \n",
       "2                        689.1                        11   \n",
       "3                         39.9                         1   \n",
       "4                        714.1                         9   \n",
       "...                        ...                       ...   \n",
       "531165                   748.2                         9   \n",
       "531166                   748.2                         9   \n",
       "531167                   239.7                         3   \n",
       "531168                   119.8                         2   \n",
       "531169                   119.8                         2   \n",
       "\n",
       "        item_price_user_id_median  item_price_user_id_mad  \\\n",
       "0                            69.9                   16.52   \n",
       "1                            69.9                   16.52   \n",
       "2                            69.9                   16.52   \n",
       "3                            39.9                    0.00   \n",
       "4                            69.9                   38.27   \n",
       "...                           ...                     ...   \n",
       "531165                       79.9                   15.42   \n",
       "531166                       79.9                   15.42   \n",
       "531167                       79.9                    0.00   \n",
       "531168                       59.9                    0.00   \n",
       "531169                       59.9                    0.00   \n",
       "\n",
       "        item_price_brand_id_mean  item_price_brand_id_std  \\\n",
       "0                          55.23                    13.65   \n",
       "1                          74.54                    30.32   \n",
       "2                          74.54                    30.32   \n",
       "3                          55.16                    19.34   \n",
       "4                          41.11                    13.08   \n",
       "...                          ...                      ...   \n",
       "531165                     64.66                    20.40   \n",
       "531166                     64.66                    20.40   \n",
       "531167                     64.66                    20.40   \n",
       "531168                     64.66                    20.40   \n",
       "531169                     64.66                    20.40   \n",
       "\n",
       "        item_price_brand_id_min  item_price_brand_id_max  \\\n",
       "0                          24.9                    79.90   \n",
       "1                          24.9                   179.95   \n",
       "2                          24.9                   179.95   \n",
       "3                           0.0                    89.90   \n",
       "4                          19.9                    79.90   \n",
       "...                         ...                      ...   \n",
       "531165                      0.0                   169.90   \n",
       "531166                      0.0                   169.90   \n",
       "531167                      0.0                   169.90   \n",
       "531168                      0.0                   169.90   \n",
       "531169                      0.0                   169.90   \n",
       "\n",
       "        item_price_brand_id_sum  item_price_brand_id_count  \\\n",
       "0                     282536.19                       5116   \n",
       "1                     478991.63                       6426   \n",
       "2                     478991.63                       6426   \n",
       "3                     629751.00                      11417   \n",
       "4                      99614.71                       2423   \n",
       "...                         ...                        ...   \n",
       "531165               2941212.79                      45487   \n",
       "531166               2941212.79                      45487   \n",
       "531167               2941212.79                      45487   \n",
       "531168               2941212.79                      45487   \n",
       "531169               2941212.79                      45487   \n",
       "\n",
       "        item_price_brand_id_median  item_price_brand_id_mad  mode_item_id  \\\n",
       "0                            59.90                    11.66            71   \n",
       "1                            69.95                    22.66            71   \n",
       "2                            69.95                    22.66            71   \n",
       "3                            49.90                    16.21            22   \n",
       "4                            39.90                    10.10            15   \n",
       "...                            ...                      ...           ...   \n",
       "531165                       69.90                    15.02          2505   \n",
       "531166                       69.90                    15.02          2505   \n",
       "531167                       69.90                    15.02          2470   \n",
       "531168                       69.90                    15.02          2452   \n",
       "531169                       69.90                    15.02          2452   \n",
       "\n",
       "       mode_size  mode_brand_id mode_item_color  \\\n",
       "0              s             20           white   \n",
       "1              s             20           white   \n",
       "2              s             20           white   \n",
       "3              s             14           green   \n",
       "4             xl             37           black   \n",
       "...          ...            ...             ...   \n",
       "531165         s              5             red   \n",
       "531166         s              5             red   \n",
       "531167         l              5           ocher   \n",
       "531168         m              5           black   \n",
       "531169         m              5           black   \n",
       "\n",
       "        price-item_price_item_id_mean  price-item_price_item_id_min  \\\n",
       "0                               -0.43                         -30.0   \n",
       "1                              -12.00                         -35.0   \n",
       "2                              -12.00                         -35.0   \n",
       "3                               -8.80                         -20.0   \n",
       "4                               -2.31                          -5.0   \n",
       "...                               ...                           ...   \n",
       "531165                          -7.66                         -20.0   \n",
       "531166                           1.89                          -5.0   \n",
       "531167                           0.00                           0.0   \n",
       "531168                           0.00                           0.0   \n",
       "531169                           0.00                           0.0   \n",
       "\n",
       "        price-item_price_item_id_max  price-item_price_user_id_mean  \\\n",
       "0                                0.0                          -7.25   \n",
       "1                                0.0                          -7.30   \n",
       "2                                0.0                          -7.30   \n",
       "3                                0.0                           0.00   \n",
       "4                               20.0                          49.44   \n",
       "...                              ...                            ...   \n",
       "531165                           0.0                          13.23   \n",
       "531166                           5.0                          18.23   \n",
       "531167                           0.0                           0.00   \n",
       "531168                           0.0                           0.00   \n",
       "531169                           0.0                           0.00   \n",
       "\n",
       "        price-item_price_user_id_min  price-item_price_user_id_max  \\\n",
       "0                             -69.90                         20.00   \n",
       "1                             -69.95                         19.95   \n",
       "2                             -69.95                         19.95   \n",
       "3                               0.00                          0.00   \n",
       "4                               0.00                        110.00   \n",
       "...                              ...                           ...   \n",
       "531165                         -5.00                         59.10   \n",
       "531166                          0.00                         64.10   \n",
       "531167                          0.00                          0.00   \n",
       "531168                          0.00                          0.00   \n",
       "531169                          0.00                          0.00   \n",
       "\n",
       "        price-item_price_brand_id_mean  price-item_price_brand_id_min  \\\n",
       "0                               -14.67                         -45.00   \n",
       "1                                 4.59                         -45.05   \n",
       "2                                 4.59                         -45.05   \n",
       "3                                15.26                         -39.90   \n",
       "4                                11.21                         -10.00   \n",
       "...                                ...                            ...   \n",
       "531165                           -5.24                         -69.90   \n",
       "531166                           -0.24                         -64.90   \n",
       "531167                          -15.24                         -79.90   \n",
       "531168                            4.76                         -59.90   \n",
       "531169                            4.76                         -59.90   \n",
       "\n",
       "        price-item_price_brand_id_max  \n",
       "0                                10.0  \n",
       "1                               110.0  \n",
       "2                               110.0  \n",
       "3                                50.0  \n",
       "4                                50.0  \n",
       "...                               ...  \n",
       "531165                          100.0  \n",
       "531166                          105.0  \n",
       "531167                           90.0  \n",
       "531168                          110.0  \n",
       "531169                          110.0  \n",
       "\n",
       "[531170 rows x 97 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(\"df_97_columns.csv\")\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a class for all the models\n",
    "class Model_class(object):\n",
    "    def __init__(self, df:dict):\n",
    "        self.df = df\n",
    "        \n",
    "        \n",
    "    def split_data(self, historic:bool)->tuple:\n",
    "        \"\"\"\n",
    "        Split the data into train and test sets, depending on whether the encdoder needs to be fited on the full data or not.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        historic : bool\n",
    "            False if the encoder needs to be fited on the Months April to February and to transform March\n",
    "            True if the encoder needs to be fited on the Months April to December and to transform January to March\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple\n",
    "            df_train : dict\n",
    "                Dataframe with training data.\n",
    "            df_test : dict\n",
    "                Dataframe with testing data.\n",
    "        \"\"\"\n",
    "        # Get list of the months to train, remove months on which to test        \n",
    "        if historic:\n",
    "            months_to_train = list(range(1,13))\n",
    "            # remove months 12,1,2,3 from the train set\n",
    "            months_to_train = [i for i in months_to_train if i not in [12,1,2,3]]\n",
    "        else:\n",
    "            months_to_train = list(range(1,13))\n",
    "            months_to_train.remove(3)\n",
    "        k = max(self.df[\"order_item_id\"])-1\n",
    "        # Split into train and test. \"~\" in front of a variable means \"not\"\n",
    "        df_train = self.df.loc[:k][self.df.loc[:k,\"order_month\"].isin(months_to_train)]\n",
    "        df_test = self.df.loc[:k][~self.df.loc[:k,\"order_month\"].isin(months_to_train)]\n",
    "        # Get the validation set\n",
    "        # df_valid = self.df.iloc[k+1:, :]\n",
    "        # Drop unnecessary columns\n",
    "        columns_to_drop = [\"order_date\", \"delivery_date\", \"user_dob\", \"user_reg_date\", \"order_id\",\"order_item_id\"]\n",
    "        real_class = pd.read_csv(\"orders_realclass.txt\", delimiter=\";\")\n",
    "        df_valid = self.df.iloc[k+1:, :]\n",
    "        df_valid.reset_index(drop=True,inplace=True)\n",
    "        df_valid[\"return\"] = real_class[\"returnShipment\"]\n",
    "        \n",
    "        df_train.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        df_test.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        df_valid.drop(columns_to_drop, axis=1, inplace=True)\n",
    "        return df_train, df_test, df_valid\n",
    "    \n",
    "    def LOE_Encoder(self, df_train:dict, df_test:dict, columns:list ,sig:float)->tuple:\n",
    "        \"\"\"\n",
    "        Leave One Out Encoder to calculate the response variable for each category.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : dict\n",
    "            Dataframe with training data.\n",
    "        df_test : dict\n",
    "            Dataframe with testing data.\n",
    "        columns : list\n",
    "            Categorical columns to encode.\n",
    "        sig : float\n",
    "            Random noise added to the response variable.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        tuple of the encoded dataframes and the encoder object.\n",
    "        \"\"\"              \n",
    "        encoder = LeaveOneOutEncoder(cols=columns, return_df=True,sigma=sig)\n",
    "        df_encode_train = encoder.fit_transform(df_train.drop([\"return\"],axis=1),df_train[[\"return\"]]).round(3)\n",
    "        df_encode_test = encoder.transform(df_test.drop([\"return\"],axis=1)).round(3)\n",
    "        df_encode_train , df_encode_test = df_encode_train.join(df_train[[\"return\"]]), df_encode_test.join(df_test[[\"return\"]])\n",
    "        return df_encode_train, df_encode_test, encoder\n",
    "    \n",
    "    def WOE_Encoder(self,df_train,df_test,columns,sig):\n",
    "        encoder = WOEEncoder(cols=columns, return_df=True,sigma=sig, verbose=True,regularization=1)\n",
    "        df_encode_train = encoder.fit_transform(df_train.drop([\"return\"],axis=1),df_train[[\"return\"]])\n",
    "        df_encode_test = encoder.transform(df_test.drop([\"return\"],axis=1))\n",
    "        df_encode_train , df_encode_test = df_encode_train.join(df_train[[\"return\"]]), df_encode_test.join(df_test[[\"return\"]])\n",
    "        return df_encode_train, df_encode_test, encoder\n",
    "    \n",
    "    def neural_network(self, df_train:dict, df_test:dict, n_layers:int, n_nodes:int, dropout:list, activation:str, optimizer:str, loss:str, metrics:list, epochs:int, batch_size:int,verbose:int)->object:\n",
    "        \"\"\"\n",
    "        Neural network model to predict whether an item will be returned or not.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        df_train : dict\n",
    "            Dataframe with training data.\n",
    "        df_test : dict\n",
    "            Dataframe with testing data.\n",
    "        n_layers : int\n",
    "            Number of layers in the neural network.\n",
    "        n_nodes : int\n",
    "            Number of nodes in each layer.\n",
    "        dropout : list\n",
    "            List of dropout rates for each layer.\n",
    "        activation : str\n",
    "            Activation function for each layer except the last one.\n",
    "        optimizer : str\n",
    "            Optimizer for the neural network.\n",
    "        loss : str\n",
    "            Loss function for the neural network.\n",
    "        metrics : str\n",
    "            List of metrics for the neural network.\n",
    "        epochs : int\n",
    "            Number of epochs for the neural network.\n",
    "        batch_size : int\n",
    "            Size of the batch\n",
    "        verbose : int\n",
    "            Whether to print the progress of the neural network\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        model: object\n",
    "            The Neural network model\n",
    "        Y_pred: array\n",
    "            Array of floats with the predictions of the neural network\n",
    "        mae: float\n",
    "            Mean absolute error on the testing set\n",
    "        \"\"\"         \n",
    "        X_train, Y_train, X_test, Y_test = self.XY_split(df_train, df_test)\n",
    "        X_train = StandardScaler().fit_transform(X_train)\n",
    "        X_test = StandardScaler().fit_transform(X_test)\n",
    "        \n",
    "        model = Sequential()\n",
    "        for i in range(n_layers):\n",
    "            if i == 0:\n",
    "                model.add(Dense(n_nodes[i], input_dim=X_train.shape[1], activation=activation))\n",
    "            else:\n",
    "                model.add(Dense(n_nodes[i], activation=activation))\n",
    "            model.add(Dropout(dropout[i]))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(optimizer=optimizer, loss=loss, metrics=metrics)\n",
    "        model.fit(X_train, Y_train, epochs=epochs, batch_size=batch_size,verbose=verbose,validation_data=(X_test, Y_test))\n",
    "        Y_pred = model.predict(X_test).round(4)\n",
    "        mae = mean_absolute_error(Y_test, Y_pred)\n",
    "        return model, Y_pred, mae,\n",
    "    \n",
    "    def xgboost(self, df_train:dict, df_test:dict, params:dict, verbose:int):\n",
    "        X_train, Y_train, X_test, Y_test = self.XY_split(df_train, df_test)\n",
    "        model = XGBClassifier(**params)\n",
    "        model.fit(X_train, Y_train, eval_metric='mae', eval_set=[(X_test, Y_test)],early_stopping_rounds = 20,verbose = verbose)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        Y_pred_prob = model.predict_proba(X_test)[:,1].round(4)\n",
    "        mae = mean_absolute_error(Y_test, Y_pred)\n",
    "        return model, Y_pred, mae, Y_pred_prob\n",
    "    \n",
    "    def catboost(self, df_train:dict, df_test:dict, params:dict, verbose:bool):\n",
    "        X_train, Y_train, X_test, Y_test = self.XY_split(df_train, df_test)\n",
    "        model = CatBoostClassifier(**params)\n",
    "        model.fit(X_train, Y_train, eval_set=(X_test, Y_test), use_best_model=True, verbose=verbose,early_stopping_rounds = 20)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        Y_pred_prob = model.predict_proba(X_test)[:,1].round(4)\n",
    "        mae = mean_absolute_error(Y_test, Y_pred)\n",
    "        return model, Y_pred, mae, Y_pred_prob\n",
    "    \n",
    "    def lightgmb(self, df_train:dict, df_test:dict, params:dict, verbose:int):\n",
    "        X_train, Y_train, X_test, Y_test = self.XY_split(df_train, df_test)\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_train, Y_train, verbose=verbose)\n",
    "        Y_pred_prob = model.predict_proba(X_test)[:,1].round(4)\n",
    "        Y_pred = model.predict(X_test)\n",
    "        mae = mean_absolute_error(Y_test, Y_pred)\n",
    "        return model, Y_pred, mae, Y_pred_prob\n",
    "    \n",
    "    def combine_models(self, prob_dict,df_test:dict):\n",
    "        prob_list = list(prob_dict.keys())\n",
    "        best_error = 1\n",
    "        for j in range(len(prob_dict)):\n",
    "            for k in range(j+1,len(prob_dict)):\n",
    "                for i in range(1,101):\n",
    "                    first_term = prob_dict[prob_list[j]]*(i/100)\n",
    "                    second_term = prob_dict[prob_list[k]]*((100-i)/100)\n",
    "                    sum = first_term + second_term\n",
    "                    error = mean_absolute_error(df_test[\"return\"], sum.round())\n",
    "                    if error < best_error:\n",
    "                        best_error = error\n",
    "                        best_i = i\n",
    "                        best_model1 = prob_list[j] + \"_\" + str(i/100)\n",
    "                        best_model2 = prob_list[k] + \"_\" + str((100-i)/100)\n",
    "                        best_combination = best_model1 + \"/\" + best_model2\n",
    "        return best_combination, best_error\n",
    "    \n",
    "    def XY_split(self, df_train:dict, df_test:dict):\n",
    "        X_train, Y_train = df_train.drop([\"return\"],axis=1), df_train[\"return\"]\n",
    "        X_test, Y_test = df_test.drop([\"return\"],axis=1), df_test[\"return\"]\n",
    "        return X_train, Y_train, X_test, Y_test\n",
    "    \n",
    "    \n",
    "    def get_feature_importance(model:object,df:dict)->list:\n",
    "        \"\"\"\n",
    "        Get feature importance from the tree-based models.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        model : object\n",
    "            Tree model from sklearn.\n",
    "        df  : dict\n",
    "            Tree model from sklearn\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            List of the features with their corresponding feature importance.\n",
    "        \"\"\"    \n",
    "        return list(sorted(zip(df.columns.drop(\"return\"), model.feature_importances_), key=lambda xx: xx[1], reverse=True))\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "Model = Model_class(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "low_cardinality_cat_cols = [\"size\",\"item_color\",\"user_title\",\"user_state\"]\n",
    "high_cardinality_cat_cols = [\"item_id\",\"brand_id\",\"user_id\"]\n",
    "cat_cols = low_cardinality_cat_cols + high_cardinality_cat_cols\n",
    "cat_cols += [\"mode_item_id\",\"mode_size\",\"mode_brand_id\",\"mode_item_color\",]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train,df_test, df_valid = Model.split_data(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_encode,df_test_encode, encoder = Model.LOE_Encoder(df_train,df_test,cat_cols,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1608/1608 [==============================] - 4s 2ms/step - loss: 0.3386 - mae: 0.3386 - val_loss: 0.3623 - val_mae: 0.3623\n",
      "Epoch 2/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3271 - mae: 0.3271 - val_loss: 0.3640 - val_mae: 0.3640\n",
      "Epoch 3/15\n",
      "1608/1608 [==============================] - 4s 2ms/step - loss: 0.3246 - mae: 0.3246 - val_loss: 0.3648 - val_mae: 0.3648\n",
      "Epoch 4/15\n",
      "1608/1608 [==============================] - 4s 2ms/step - loss: 0.3241 - mae: 0.3241 - val_loss: 0.3635 - val_mae: 0.3635\n",
      "Epoch 5/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3236 - mae: 0.3236 - val_loss: 0.3620 - val_mae: 0.3620\n",
      "Epoch 6/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3229 - mae: 0.3229 - val_loss: 0.3626 - val_mae: 0.3626\n",
      "Epoch 7/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3222 - mae: 0.3222 - val_loss: 0.3613 - val_mae: 0.3613\n",
      "Epoch 8/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3207 - mae: 0.3207 - val_loss: 0.3607 - val_mae: 0.3607\n",
      "Epoch 9/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.3583 - val_mae: 0.3583\n",
      "Epoch 10/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3202 - mae: 0.3202 - val_loss: 0.3595 - val_mae: 0.3595\n",
      "Epoch 11/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3191 - mae: 0.3191 - val_loss: 0.3598 - val_mae: 0.3598\n",
      "Epoch 12/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3188 - mae: 0.3188 - val_loss: 0.3570 - val_mae: 0.3570\n",
      "Epoch 13/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3180 - mae: 0.3180 - val_loss: 0.3588 - val_mae: 0.3588\n",
      "Epoch 14/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3175 - mae: 0.3175 - val_loss: 0.3550 - val_mae: 0.3550\n",
      "Epoch 15/15\n",
      "1608/1608 [==============================] - 3s 2ms/step - loss: 0.3165 - mae: 0.3165 - val_loss: 0.3585 - val_mae: 0.3585\n",
      "Neural Network MAE: 0.3584869367945933\n"
     ]
    }
   ],
   "source": [
    "neural_network_params = {\"n_layers\":2, \"n_nodes\":[64,64], \"dropout\":[0.1,0.1], \"activation\":\"relu\", \"optimizer\":\"adam\", \"loss\":\"mae\", \"metrics\":[\"mae\"], \"epochs\":15, \"batch_size\":256, \"verbose\":1}\n",
    "neural_network_model, nn_Y_pred, nn_mae = Model.neural_network(df_train_encode, df_test_encode, **neural_network_params)\n",
    "print(\"Neural Network MAE:\",nn_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = { \"max_depth\":9, \"learning_rate\":0.16, \"n_estimators\":50, \"objective\":\"binary:logistic\",\"random_state\":42}\n",
    "xgb_model, xgb_Y_pred, xgb_mae, xgb_Y_pred_prob = Model.xgboost(df_train_encode, df_test_encode, xgb_params, 1)\n",
    "print(\"XGB MAE\",xgb_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list_of_errors = []\n",
    "# for i in range(1,101):\n",
    "#     xgb_Y_pred_prob_threshold = xgb_Y_pred_prob.copy()\n",
    "#     xgb_Y_pred_prob_threshold[xgb_Y_pred_prob_threshold>=i/100] = 1\n",
    "#     xgb_Y_pred_prob_threshold[xgb_Y_pred_prob_threshold<i/100] = 0\n",
    "#     # mean_absolute_error(df_test[\"return\"],xgb_Y_pred_prob_threshold)\n",
    "#     list_of_errors.append(mean_absolute_error(df_test[\"return\"],xgb_Y_pred_prob_threshold))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6629709\ttest: 0.6723072\tbest: 0.6723072 (0)\ttotal: 225ms\tremaining: 22.3s\n",
      "1:\tlearn: 0.6432539\ttest: 0.6586959\tbest: 0.6586959 (1)\ttotal: 399ms\tremaining: 19.5s\n",
      "2:\tlearn: 0.6282367\ttest: 0.6496332\tbest: 0.6496332 (2)\ttotal: 584ms\tremaining: 18.9s\n",
      "3:\tlearn: 0.6172558\ttest: 0.6428240\tbest: 0.6428240 (3)\ttotal: 761ms\tremaining: 18.3s\n",
      "4:\tlearn: 0.6075434\ttest: 0.6342311\tbest: 0.6342311 (4)\ttotal: 968ms\tremaining: 18.4s\n",
      "5:\tlearn: 0.6015414\ttest: 0.6307368\tbest: 0.6307368 (5)\ttotal: 1.14s\tremaining: 17.9s\n",
      "6:\tlearn: 0.5953330\ttest: 0.6263933\tbest: 0.6263933 (6)\ttotal: 1.33s\tremaining: 17.7s\n",
      "7:\tlearn: 0.5918784\ttest: 0.6243011\tbest: 0.6243011 (7)\ttotal: 1.5s\tremaining: 17.3s\n",
      "8:\tlearn: 0.5881345\ttest: 0.6222308\tbest: 0.6222308 (8)\ttotal: 1.69s\tremaining: 17.1s\n",
      "9:\tlearn: 0.5851364\ttest: 0.6199034\tbest: 0.6199034 (9)\ttotal: 1.88s\tremaining: 16.9s\n",
      "10:\tlearn: 0.5830651\ttest: 0.6186545\tbest: 0.6186545 (10)\ttotal: 2.05s\tremaining: 16.6s\n",
      "11:\tlearn: 0.5810257\ttest: 0.6179720\tbest: 0.6179720 (11)\ttotal: 2.37s\tremaining: 17.4s\n",
      "12:\tlearn: 0.5798839\ttest: 0.6174413\tbest: 0.6174413 (12)\ttotal: 2.56s\tremaining: 17.1s\n",
      "13:\tlearn: 0.5783458\ttest: 0.6169022\tbest: 0.6169022 (13)\ttotal: 2.77s\tremaining: 17s\n",
      "14:\tlearn: 0.5774150\ttest: 0.6164341\tbest: 0.6164341 (14)\ttotal: 2.94s\tremaining: 16.7s\n",
      "15:\tlearn: 0.5766236\ttest: 0.6156967\tbest: 0.6156967 (15)\ttotal: 3.13s\tremaining: 16.5s\n",
      "16:\tlearn: 0.5755543\ttest: 0.6145739\tbest: 0.6145739 (16)\ttotal: 3.31s\tremaining: 16.1s\n",
      "17:\tlearn: 0.5748641\ttest: 0.6145949\tbest: 0.6145739 (16)\ttotal: 3.49s\tremaining: 15.9s\n",
      "18:\tlearn: 0.5742419\ttest: 0.6145440\tbest: 0.6145440 (18)\ttotal: 3.65s\tremaining: 15.6s\n",
      "19:\tlearn: 0.5732972\ttest: 0.6134126\tbest: 0.6134126 (19)\ttotal: 3.84s\tremaining: 15.4s\n",
      "20:\tlearn: 0.5721646\ttest: 0.6131238\tbest: 0.6131238 (20)\ttotal: 4.08s\tremaining: 15.4s\n",
      "21:\tlearn: 0.5716016\ttest: 0.6128591\tbest: 0.6128591 (21)\ttotal: 4.28s\tremaining: 15.2s\n",
      "22:\tlearn: 0.5710159\ttest: 0.6129022\tbest: 0.6128591 (21)\ttotal: 4.46s\tremaining: 14.9s\n",
      "23:\tlearn: 0.5706585\ttest: 0.6127686\tbest: 0.6127686 (23)\ttotal: 4.64s\tremaining: 14.7s\n",
      "24:\tlearn: 0.5701627\ttest: 0.6120948\tbest: 0.6120948 (24)\ttotal: 4.81s\tremaining: 14.4s\n",
      "25:\tlearn: 0.5696334\ttest: 0.6118182\tbest: 0.6118182 (25)\ttotal: 4.99s\tremaining: 14.2s\n",
      "26:\tlearn: 0.5692619\ttest: 0.6116582\tbest: 0.6116582 (26)\ttotal: 5.17s\tremaining: 14s\n",
      "27:\tlearn: 0.5689001\ttest: 0.6116442\tbest: 0.6116442 (27)\ttotal: 5.35s\tremaining: 13.8s\n",
      "28:\tlearn: 0.5684833\ttest: 0.6113705\tbest: 0.6113705 (28)\ttotal: 5.64s\tremaining: 13.8s\n",
      "29:\tlearn: 0.5680252\ttest: 0.6115519\tbest: 0.6113705 (28)\ttotal: 6.04s\tremaining: 14.1s\n",
      "30:\tlearn: 0.5677355\ttest: 0.6114778\tbest: 0.6113705 (28)\ttotal: 6.37s\tremaining: 14.2s\n",
      "31:\tlearn: 0.5673632\ttest: 0.6115397\tbest: 0.6113705 (28)\ttotal: 6.94s\tremaining: 14.7s\n",
      "32:\tlearn: 0.5667570\ttest: 0.6110368\tbest: 0.6110368 (32)\ttotal: 7.25s\tremaining: 14.7s\n",
      "33:\tlearn: 0.5662415\ttest: 0.6114816\tbest: 0.6110368 (32)\ttotal: 7.64s\tremaining: 14.8s\n",
      "34:\tlearn: 0.5660013\ttest: 0.6114439\tbest: 0.6110368 (32)\ttotal: 8.02s\tremaining: 14.9s\n",
      "35:\tlearn: 0.5654546\ttest: 0.6109923\tbest: 0.6109923 (35)\ttotal: 8.49s\tremaining: 15.1s\n",
      "36:\tlearn: 0.5651949\ttest: 0.6108634\tbest: 0.6108634 (36)\ttotal: 9.13s\tremaining: 15.5s\n",
      "37:\tlearn: 0.5650497\ttest: 0.6107748\tbest: 0.6107748 (37)\ttotal: 9.64s\tremaining: 15.7s\n",
      "38:\tlearn: 0.5645962\ttest: 0.6112796\tbest: 0.6107748 (37)\ttotal: 10s\tremaining: 15.6s\n",
      "39:\tlearn: 0.5642654\ttest: 0.6112218\tbest: 0.6107748 (37)\ttotal: 10.3s\tremaining: 15.5s\n",
      "40:\tlearn: 0.5639900\ttest: 0.6111828\tbest: 0.6107748 (37)\ttotal: 10.6s\tremaining: 15.3s\n",
      "41:\tlearn: 0.5637355\ttest: 0.6111528\tbest: 0.6107748 (37)\ttotal: 11s\tremaining: 15.1s\n",
      "42:\tlearn: 0.5635773\ttest: 0.6110662\tbest: 0.6107748 (37)\ttotal: 11.2s\tremaining: 14.8s\n",
      "43:\tlearn: 0.5634509\ttest: 0.6110576\tbest: 0.6107748 (37)\ttotal: 11.4s\tremaining: 14.5s\n",
      "44:\tlearn: 0.5629537\ttest: 0.6106763\tbest: 0.6106763 (44)\ttotal: 11.6s\tremaining: 14.2s\n",
      "45:\tlearn: 0.5626782\ttest: 0.6106192\tbest: 0.6106192 (45)\ttotal: 11.8s\tremaining: 13.8s\n",
      "46:\tlearn: 0.5624175\ttest: 0.6105194\tbest: 0.6105194 (46)\ttotal: 12s\tremaining: 13.5s\n",
      "47:\tlearn: 0.5621324\ttest: 0.6105076\tbest: 0.6105076 (47)\ttotal: 12.2s\tremaining: 13.2s\n",
      "48:\tlearn: 0.5617081\ttest: 0.6103733\tbest: 0.6103733 (48)\ttotal: 12.4s\tremaining: 12.9s\n",
      "49:\tlearn: 0.5613611\ttest: 0.6101188\tbest: 0.6101188 (49)\ttotal: 12.6s\tremaining: 12.6s\n",
      "50:\tlearn: 0.5612028\ttest: 0.6101438\tbest: 0.6101188 (49)\ttotal: 12.7s\tremaining: 12.2s\n",
      "51:\tlearn: 0.5609522\ttest: 0.6101481\tbest: 0.6101188 (49)\ttotal: 12.9s\tremaining: 11.9s\n",
      "52:\tlearn: 0.5607512\ttest: 0.6101508\tbest: 0.6101188 (49)\ttotal: 13.1s\tremaining: 11.6s\n",
      "53:\tlearn: 0.5605412\ttest: 0.6101273\tbest: 0.6101188 (49)\ttotal: 13.2s\tremaining: 11.3s\n",
      "54:\tlearn: 0.5602023\ttest: 0.6099844\tbest: 0.6099844 (54)\ttotal: 13.4s\tremaining: 11s\n",
      "55:\tlearn: 0.5600660\ttest: 0.6099090\tbest: 0.6099090 (55)\ttotal: 13.6s\tremaining: 10.7s\n",
      "56:\tlearn: 0.5599065\ttest: 0.6098592\tbest: 0.6098592 (56)\ttotal: 13.7s\tremaining: 10.4s\n",
      "57:\tlearn: 0.5596835\ttest: 0.6098215\tbest: 0.6098215 (57)\ttotal: 13.9s\tremaining: 10.1s\n",
      "58:\tlearn: 0.5594016\ttest: 0.6098230\tbest: 0.6098215 (57)\ttotal: 14.1s\tremaining: 9.79s\n",
      "59:\tlearn: 0.5590726\ttest: 0.6099944\tbest: 0.6098215 (57)\ttotal: 14.3s\tremaining: 9.55s\n",
      "60:\tlearn: 0.5589134\ttest: 0.6099340\tbest: 0.6098215 (57)\ttotal: 14.5s\tremaining: 9.3s\n",
      "61:\tlearn: 0.5586509\ttest: 0.6103367\tbest: 0.6098215 (57)\ttotal: 14.8s\tremaining: 9.05s\n",
      "62:\tlearn: 0.5583768\ttest: 0.6101756\tbest: 0.6098215 (57)\ttotal: 15s\tremaining: 8.8s\n",
      "63:\tlearn: 0.5581604\ttest: 0.6099490\tbest: 0.6098215 (57)\ttotal: 15.2s\tremaining: 8.55s\n",
      "64:\tlearn: 0.5580602\ttest: 0.6099302\tbest: 0.6098215 (57)\ttotal: 15.4s\tremaining: 8.29s\n",
      "65:\tlearn: 0.5577967\ttest: 0.6098246\tbest: 0.6098215 (57)\ttotal: 15.6s\tremaining: 8.03s\n",
      "66:\tlearn: 0.5576056\ttest: 0.6097718\tbest: 0.6097718 (66)\ttotal: 15.8s\tremaining: 7.8s\n",
      "67:\tlearn: 0.5574331\ttest: 0.6097786\tbest: 0.6097718 (66)\ttotal: 16s\tremaining: 7.53s\n",
      "68:\tlearn: 0.5571440\ttest: 0.6098077\tbest: 0.6097718 (66)\ttotal: 16.2s\tremaining: 7.28s\n",
      "69:\tlearn: 0.5568766\ttest: 0.6097892\tbest: 0.6097718 (66)\ttotal: 16.4s\tremaining: 7.02s\n",
      "70:\tlearn: 0.5566009\ttest: 0.6099521\tbest: 0.6097718 (66)\ttotal: 16.6s\tremaining: 6.76s\n",
      "71:\tlearn: 0.5563846\ttest: 0.6099731\tbest: 0.6097718 (66)\ttotal: 16.7s\tremaining: 6.51s\n",
      "72:\tlearn: 0.5562495\ttest: 0.6098882\tbest: 0.6097718 (66)\ttotal: 16.9s\tremaining: 6.25s\n",
      "73:\tlearn: 0.5560057\ttest: 0.6098954\tbest: 0.6097718 (66)\ttotal: 17.1s\tremaining: 6s\n",
      "74:\tlearn: 0.5558114\ttest: 0.6100790\tbest: 0.6097718 (66)\ttotal: 17.2s\tremaining: 5.75s\n",
      "75:\tlearn: 0.5556407\ttest: 0.6100911\tbest: 0.6097718 (66)\ttotal: 17.4s\tremaining: 5.5s\n",
      "76:\tlearn: 0.5554669\ttest: 0.6100877\tbest: 0.6097718 (66)\ttotal: 17.6s\tremaining: 5.25s\n",
      "77:\tlearn: 0.5553179\ttest: 0.6100487\tbest: 0.6097718 (66)\ttotal: 17.8s\tremaining: 5.01s\n",
      "78:\tlearn: 0.5551152\ttest: 0.6099299\tbest: 0.6097718 (66)\ttotal: 18s\tremaining: 4.77s\n",
      "79:\tlearn: 0.5549086\ttest: 0.6099380\tbest: 0.6097718 (66)\ttotal: 18.1s\tremaining: 4.53s\n",
      "80:\tlearn: 0.5546500\ttest: 0.6099687\tbest: 0.6097718 (66)\ttotal: 18.3s\tremaining: 4.3s\n",
      "81:\tlearn: 0.5545490\ttest: 0.6099440\tbest: 0.6097718 (66)\ttotal: 18.5s\tremaining: 4.06s\n",
      "82:\tlearn: 0.5543554\ttest: 0.6098873\tbest: 0.6097718 (66)\ttotal: 18.7s\tremaining: 3.84s\n",
      "83:\tlearn: 0.5540469\ttest: 0.6095654\tbest: 0.6095654 (83)\ttotal: 18.9s\tremaining: 3.61s\n",
      "84:\tlearn: 0.5537874\ttest: 0.6093872\tbest: 0.6093872 (84)\ttotal: 19.1s\tremaining: 3.37s\n",
      "85:\tlearn: 0.5534020\ttest: 0.6093510\tbest: 0.6093510 (85)\ttotal: 19.3s\tremaining: 3.14s\n",
      "86:\tlearn: 0.5531722\ttest: 0.6093548\tbest: 0.6093510 (85)\ttotal: 19.5s\tremaining: 2.91s\n",
      "87:\tlearn: 0.5529356\ttest: 0.6094600\tbest: 0.6093510 (85)\ttotal: 19.6s\tremaining: 2.68s\n",
      "88:\tlearn: 0.5527332\ttest: 0.6094745\tbest: 0.6093510 (85)\ttotal: 19.8s\tremaining: 2.45s\n",
      "89:\tlearn: 0.5525091\ttest: 0.6094613\tbest: 0.6093510 (85)\ttotal: 20s\tremaining: 2.22s\n",
      "90:\tlearn: 0.5523337\ttest: 0.6094633\tbest: 0.6093510 (85)\ttotal: 20.2s\tremaining: 2s\n",
      "91:\tlearn: 0.5520533\ttest: 0.6096817\tbest: 0.6093510 (85)\ttotal: 20.4s\tremaining: 1.77s\n",
      "92:\tlearn: 0.5517739\ttest: 0.6095376\tbest: 0.6093510 (85)\ttotal: 20.6s\tremaining: 1.55s\n",
      "93:\tlearn: 0.5515884\ttest: 0.6095275\tbest: 0.6093510 (85)\ttotal: 20.8s\tremaining: 1.32s\n",
      "94:\tlearn: 0.5513639\ttest: 0.6095344\tbest: 0.6093510 (85)\ttotal: 20.9s\tremaining: 1.1s\n",
      "95:\tlearn: 0.5511592\ttest: 0.6095318\tbest: 0.6093510 (85)\ttotal: 21.3s\tremaining: 886ms\n",
      "96:\tlearn: 0.5509570\ttest: 0.6094530\tbest: 0.6093510 (85)\ttotal: 21.4s\tremaining: 663ms\n",
      "97:\tlearn: 0.5507816\ttest: 0.6094275\tbest: 0.6093510 (85)\ttotal: 21.6s\tremaining: 441ms\n",
      "98:\tlearn: 0.5505879\ttest: 0.6094427\tbest: 0.6093510 (85)\ttotal: 21.8s\tremaining: 220ms\n",
      "99:\tlearn: 0.5503909\ttest: 0.6093989\tbest: 0.6093510 (85)\ttotal: 21.9s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.6093510057\n",
      "bestIteration = 85\n",
      "\n",
      "Shrink model to first 86 iterations.\n",
      "Catboost MAE 0.33166298136289235\n"
     ]
    }
   ],
   "source": [
    "catboost_params = { \"depth\":9, \"random_seed\":42, \"iterations\":100, \"loss_function\":\"Logloss\", \"verbose\":1,\"learning_rate\":0.15}\n",
    "catboost_model, catboost_Y_pred, catboost_mae, catboost_Y_pred_prob = Model.catboost(df_train_encode, df_test_encode, catboost_params, 1)\n",
    "print(\"Catboost MAE\",catboost_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightgbm MAE 0.33417568848175055\n"
     ]
    }
   ],
   "source": [
    "lightgbm_params = { \"learning_rate\":0.15, \"n_estimators\":50, \"objective\":\"binary\",\"random_state\":42}\n",
    "lightgbm_model, lightgbm_Y_pred, lightgbm_mae, lightgbm_Y_pred_prob = Model.lightgmb(df_train_encode, df_test_encode, lightgbm_params, 1)\n",
    "print(\"Lightgbm MAE\",lightgbm_mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to determine the best model through calculating the threshold to combine the predictions of the two models\n",
    "prob_dict = {\"xgb\":xgb_Y_pred_prob, \"catboost\":catboost_Y_pred_prob, \"lightgbm\":lightgbm_Y_pred_prob, \"neural_network\":nn_Y_pred[:,0]}\n",
    "def combine_models(prob_dict):\n",
    "    prob_list = list(prob_dict.keys())\n",
    "    error_best = 1\n",
    "    for j in range(len(prob_dict)):\n",
    "        for k in range(j+1,len(prob_dict)):\n",
    "            for i in range(1,101):\n",
    "                first_term = prob_dict[prob_list[j]]*(i/100)\n",
    "                second_term = prob_dict[prob_list[k]]*((100-i)/100)\n",
    "                sum = first_term + second_term\n",
    "                error = mean_absolute_error(df_test[\"return\"], sum.round())\n",
    "                if error < error_best:\n",
    "                    error_best = error\n",
    "                    best_i = i\n",
    "                    best_model1 = prob_list[j] + \"_\" + str(i/100)\n",
    "                    best_model2 = prob_list[k] + \"_\" + str((100-i)/100)\n",
    "                    best_combination = best_model1 + \"/\" + best_model2\n",
    "    return error_best, best_combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Combination xgb_0.6/catboost_0.4\n"
     ]
    }
   ],
   "source": [
    "prob_dict = {\"xgb\":xgb_Y_pred_prob, \"catboost\":catboost_Y_pred_prob, \"lightgbm\":lightgbm_Y_pred_prob, \"neural_network\":nn_Y_pred[:,0]}\n",
    "best_combination, best_combination_mae = Model.combine_models(prob_dict,df_test_encode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MAE_test_set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>xgb_0.6/catboost_0.4</th>\n",
       "      <td>0.3305</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Catboost</th>\n",
       "      <td>0.3317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Lightgbm</th>\n",
       "      <td>0.3342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>XGB</th>\n",
       "      <td>0.3343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Neural Network</th>\n",
       "      <td>0.3585</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      MAE_test_set\n",
       "xgb_0.6/catboost_0.4        0.3305\n",
       "Catboost                    0.3317\n",
       "Lightgbm                    0.3342\n",
       "XGB                         0.3343\n",
       "Neural Network              0.3585"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAE_dict = {\"Neural Network\":nn_mae,\"XGB\":xgb_mae,\"Catboost\":catboost_mae,\"Lightgbm\":lightgbm_mae,str(best_combination):best_combination_mae}\n",
    "MAE_df = pd.DataFrame(MAE_dict,index=[\"MAE_test_set\"])\n",
    "MAE_df.sort_values(by=\"MAE_test_set\",ascending=True,inplace=True,axis=1)\n",
    "MAE_df = round(MAE_df.transpose(),4) \n",
    "MAE_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df_train and df_test\n",
    "df_train_conc = pd.concat([df_train,df_test],axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_conc_encode, df_valid_encode, encoder = Model.LOE_Encoder(df_train_conc,df_valid,cat_cols,0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3364 - mae: 0.3364 - val_loss: 0.3545 - val_mae: 0.3545\n",
      "Epoch 2/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3262 - mae: 0.3262 - val_loss: 0.3540 - val_mae: 0.3540\n",
      "Epoch 3/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3241 - mae: 0.3241 - val_loss: 0.3537 - val_mae: 0.3537s: 0.3243 - mae: 0. - ETA: 0s - loss: 0.3240 - mae: 0.3\n",
      "Epoch 4/15\n",
      "1880/1880 [==============================] - 5s 3ms/step - loss: 0.3237 - mae: 0.3237 - val_loss: 0.3544 - val_mae: 0.3544\n",
      "Epoch 5/15\n",
      "1880/1880 [==============================] - 5s 2ms/step - loss: 0.3223 - mae: 0.3223 - val_loss: 0.3479 - val_mae: 0.3479\n",
      "Epoch 6/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3207 - mae: 0.3207 - val_loss: 0.3479 - val_mae: 0.3479\n",
      "Epoch 7/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3201 - mae: 0.3201 - val_loss: 0.3481 - val_mae: 0.3481\n",
      "Epoch 8/15\n",
      "1880/1880 [==============================] - 4s 2ms/step - loss: 0.3185 - mae: 0.3185 - val_loss: 0.3448 - val_mae: 0.3448\n",
      "Epoch 9/15\n",
      "1880/1880 [==============================] - 4s 2ms/step - loss: 0.3174 - mae: 0.3174 - val_loss: 0.3424 - val_mae: 0.3424\n",
      "Epoch 10/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3164 - mae: 0.3164 - val_loss: 0.3428 - val_mae: 0.3428\n",
      "Epoch 11/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3154 - mae: 0.3154 - val_loss: 0.3394 - val_mae: 0.3394\n",
      "Epoch 12/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3143 - mae: 0.3143 - val_loss: 0.3435 - val_mae: 0.3435\n",
      "Epoch 13/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3136 - mae: 0.3136 - val_loss: 0.3402 - val_mae: 0.3402\n",
      "Epoch 14/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3126 - mae: 0.3126 - val_loss: 0.3403 - val_mae: 0.3403\n",
      "Epoch 15/15\n",
      "1880/1880 [==============================] - 3s 2ms/step - loss: 0.3126 - mae: 0.3126 - val_loss: 0.3395 - val_mae: 0.3395\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.33952204361467275"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "neural_network_val, nn_Y_pred_val, nn_mae_val = Model.neural_network(df_train_conc_encode, df_valid_encode, **neural_network_params)\n",
    "nn_mae_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb_params = { \"max_depth\":9, \"learning_rate\":0.16, \"n_estimators\":50, \"objective\":\"binary:logistic\",\"random_state\":42}\n",
    "xgb_model_val, xgb_Y_pred_val, xgb_mae_val, xgb_Y_pred_prob_val = Model.xgboost(df_train_conc_encode, df_valid_encode, xgb_params, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGB MAE 0.3175645992252087\n"
     ]
    }
   ],
   "source": [
    "print(\"XGB MAE\",xgb_mae_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Catboost MAE 0.32155836894444667\n"
     ]
    }
   ],
   "source": [
    "catboost_model_val, catboost_Y_pred_val, catboost_mae_val, catboost_Y_pred_prob_val = Model.catboost(df_train_conc_encode, df_valid_encode, catboost_params, 0)\n",
    "print(\"Catboost MAE\",catboost_mae_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lightgbm MAE 0.3201605495427134\n"
     ]
    }
   ],
   "source": [
    "lightgbm_model_val, lightgbm_Y_pred_val, lightgbm_mae_val, lightgbm_Y_pred_prob_val = Model.lightgmb(df_train_conc_encode, df_valid_encode, lightgbm_params, 0)\n",
    "print(\"Lightgbm MAE\",lightgbm_mae_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_prob = xgb_Y_pred_prob_val*(0.6) + catboost_Y_pred_prob_val*(0.4)\n",
    "combined_prob = combined_prob.round()\n",
    "combined_error = mean_absolute_error(df_valid[\"return\"],combined_prob).round(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15903.0"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_error = 50078*xgb_mae_val\n",
    "total_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_historical, df_test_historical, df_valid = Model.split_data(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_historical_encode, df_test_historical_encode, encoder = Model.WOE_Encoder(df_train_historical,df_test_historical,cat_cols,0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "months_to_train = [12,1,2]\n",
    "df_test_historical_encode_train = df_test_historical_encode[df_test_historical_encode[\"order_month\"].isin(months_to_train)]\n",
    "df_test_historical_encode_test = df_test_historical_encode[~df_test_historical_encode[\"order_month\"].isin(months_to_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0]\tvalidation_0-mae:0.49014\n",
      "[1]\tvalidation_0-mae:0.48140\n",
      "[2]\tvalidation_0-mae:0.47436\n",
      "[3]\tvalidation_0-mae:0.46856\n",
      "[4]\tvalidation_0-mae:0.46244\n",
      "[5]\tvalidation_0-mae:0.45726\n",
      "[6]\tvalidation_0-mae:0.45384\n",
      "[7]\tvalidation_0-mae:0.45042\n",
      "[8]\tvalidation_0-mae:0.44746\n",
      "[9]\tvalidation_0-mae:0.44405\n",
      "[10]\tvalidation_0-mae:0.44149\n",
      "[11]\tvalidation_0-mae:0.43895\n",
      "[12]\tvalidation_0-mae:0.43694\n",
      "[13]\tvalidation_0-mae:0.43486\n",
      "[14]\tvalidation_0-mae:0.43343\n",
      "[15]\tvalidation_0-mae:0.43219\n",
      "[16]\tvalidation_0-mae:0.43128\n",
      "[17]\tvalidation_0-mae:0.43062\n",
      "[18]\tvalidation_0-mae:0.42941\n",
      "[19]\tvalidation_0-mae:0.42808\n",
      "[20]\tvalidation_0-mae:0.42740\n",
      "[21]\tvalidation_0-mae:0.42679\n",
      "[22]\tvalidation_0-mae:0.42622\n",
      "[23]\tvalidation_0-mae:0.42555\n",
      "[24]\tvalidation_0-mae:0.42516\n",
      "[25]\tvalidation_0-mae:0.42439\n",
      "[26]\tvalidation_0-mae:0.42348\n",
      "[27]\tvalidation_0-mae:0.42299\n",
      "[28]\tvalidation_0-mae:0.42257\n",
      "[29]\tvalidation_0-mae:0.42236\n",
      "[30]\tvalidation_0-mae:0.42207\n",
      "[31]\tvalidation_0-mae:0.42184\n",
      "[32]\tvalidation_0-mae:0.42169\n",
      "[33]\tvalidation_0-mae:0.42110\n",
      "[34]\tvalidation_0-mae:0.42093\n",
      "[35]\tvalidation_0-mae:0.42062\n",
      "[36]\tvalidation_0-mae:0.42036\n",
      "[37]\tvalidation_0-mae:0.42004\n",
      "[38]\tvalidation_0-mae:0.41920\n",
      "[39]\tvalidation_0-mae:0.41909\n",
      "[40]\tvalidation_0-mae:0.41885\n",
      "[41]\tvalidation_0-mae:0.41884\n",
      "[42]\tvalidation_0-mae:0.41864\n",
      "[43]\tvalidation_0-mae:0.41795\n",
      "[44]\tvalidation_0-mae:0.41779\n",
      "[45]\tvalidation_0-mae:0.41775\n",
      "[46]\tvalidation_0-mae:0.41767\n",
      "[47]\tvalidation_0-mae:0.41750\n",
      "[48]\tvalidation_0-mae:0.41737\n",
      "[49]\tvalidation_0-mae:0.41696\n",
      "[50]\tvalidation_0-mae:0.41668\n",
      "[51]\tvalidation_0-mae:0.41660\n",
      "[52]\tvalidation_0-mae:0.41653\n",
      "[53]\tvalidation_0-mae:0.41646\n",
      "[54]\tvalidation_0-mae:0.41640\n",
      "[55]\tvalidation_0-mae:0.41626\n",
      "[56]\tvalidation_0-mae:0.41618\n",
      "[57]\tvalidation_0-mae:0.41616\n",
      "[58]\tvalidation_0-mae:0.41601\n",
      "[59]\tvalidation_0-mae:0.41583\n",
      "[60]\tvalidation_0-mae:0.41577\n",
      "[61]\tvalidation_0-mae:0.41578\n",
      "[62]\tvalidation_0-mae:0.41526\n",
      "[63]\tvalidation_0-mae:0.41523\n",
      "[64]\tvalidation_0-mae:0.41510\n",
      "[65]\tvalidation_0-mae:0.41500\n",
      "[66]\tvalidation_0-mae:0.41492\n",
      "[67]\tvalidation_0-mae:0.41478\n",
      "[68]\tvalidation_0-mae:0.41477\n",
      "[69]\tvalidation_0-mae:0.41447\n",
      "[70]\tvalidation_0-mae:0.41439\n",
      "[71]\tvalidation_0-mae:0.41432\n",
      "[72]\tvalidation_0-mae:0.41417\n",
      "[73]\tvalidation_0-mae:0.41412\n",
      "[74]\tvalidation_0-mae:0.41376\n",
      "[75]\tvalidation_0-mae:0.41358\n",
      "[76]\tvalidation_0-mae:0.41343\n",
      "[77]\tvalidation_0-mae:0.41343\n",
      "[78]\tvalidation_0-mae:0.41337\n",
      "[79]\tvalidation_0-mae:0.41324\n",
      "[80]\tvalidation_0-mae:0.41318\n",
      "[81]\tvalidation_0-mae:0.41312\n",
      "[82]\tvalidation_0-mae:0.41315\n",
      "[83]\tvalidation_0-mae:0.41312\n",
      "[84]\tvalidation_0-mae:0.41309\n",
      "[85]\tvalidation_0-mae:0.41290\n",
      "[86]\tvalidation_0-mae:0.41282\n",
      "[87]\tvalidation_0-mae:0.41266\n",
      "[88]\tvalidation_0-mae:0.41262\n",
      "[89]\tvalidation_0-mae:0.41262\n",
      "[90]\tvalidation_0-mae:0.41260\n",
      "[91]\tvalidation_0-mae:0.41251\n",
      "[92]\tvalidation_0-mae:0.41242\n",
      "[93]\tvalidation_0-mae:0.41228\n",
      "[94]\tvalidation_0-mae:0.41226\n",
      "[95]\tvalidation_0-mae:0.41226\n",
      "[96]\tvalidation_0-mae:0.41224\n",
      "[97]\tvalidation_0-mae:0.41207\n",
      "[98]\tvalidation_0-mae:0.41204\n",
      "[99]\tvalidation_0-mae:0.41199\n",
      "[100]\tvalidation_0-mae:0.41194\n",
      "[101]\tvalidation_0-mae:0.41169\n",
      "[102]\tvalidation_0-mae:0.41155\n",
      "[103]\tvalidation_0-mae:0.41153\n",
      "[104]\tvalidation_0-mae:0.41155\n",
      "[105]\tvalidation_0-mae:0.41141\n",
      "[106]\tvalidation_0-mae:0.41135\n",
      "[107]\tvalidation_0-mae:0.41133\n",
      "[108]\tvalidation_0-mae:0.41120\n",
      "[109]\tvalidation_0-mae:0.41120\n",
      "[110]\tvalidation_0-mae:0.41112\n",
      "[111]\tvalidation_0-mae:0.41107\n",
      "[112]\tvalidation_0-mae:0.41097\n",
      "[113]\tvalidation_0-mae:0.41091\n",
      "[114]\tvalidation_0-mae:0.41063\n",
      "[115]\tvalidation_0-mae:0.41063\n",
      "[116]\tvalidation_0-mae:0.41054\n",
      "[117]\tvalidation_0-mae:0.41042\n",
      "[118]\tvalidation_0-mae:0.41036\n",
      "[119]\tvalidation_0-mae:0.41035\n",
      "[120]\tvalidation_0-mae:0.41024\n",
      "[121]\tvalidation_0-mae:0.41020\n",
      "[122]\tvalidation_0-mae:0.41015\n",
      "[123]\tvalidation_0-mae:0.41012\n",
      "[124]\tvalidation_0-mae:0.41014\n",
      "[125]\tvalidation_0-mae:0.41012\n",
      "[126]\tvalidation_0-mae:0.40999\n",
      "[127]\tvalidation_0-mae:0.40999\n",
      "[128]\tvalidation_0-mae:0.40988\n",
      "[129]\tvalidation_0-mae:0.40980\n",
      "[130]\tvalidation_0-mae:0.40974\n",
      "[131]\tvalidation_0-mae:0.40972\n",
      "[132]\tvalidation_0-mae:0.40984\n",
      "[133]\tvalidation_0-mae:0.40971\n",
      "[134]\tvalidation_0-mae:0.40967\n",
      "[135]\tvalidation_0-mae:0.40961\n",
      "[136]\tvalidation_0-mae:0.40945\n",
      "[137]\tvalidation_0-mae:0.40934\n",
      "[138]\tvalidation_0-mae:0.40929\n",
      "[139]\tvalidation_0-mae:0.40928\n",
      "[140]\tvalidation_0-mae:0.40920\n",
      "[141]\tvalidation_0-mae:0.40913\n",
      "[142]\tvalidation_0-mae:0.40899\n",
      "[143]\tvalidation_0-mae:0.40898\n",
      "[144]\tvalidation_0-mae:0.40894\n",
      "[145]\tvalidation_0-mae:0.40889\n",
      "[146]\tvalidation_0-mae:0.40889\n",
      "[147]\tvalidation_0-mae:0.40881\n",
      "[148]\tvalidation_0-mae:0.40880\n",
      "[149]\tvalidation_0-mae:0.40874\n",
      "XGB MAE 0.3323952560089596\n"
     ]
    }
   ],
   "source": [
    "xgb_params = { \"max_depth\":9, \"learning_rate\":0.16, \"n_estimators\":150, \"objective\":\"binary:logistic\",\"random_state\":42}\n",
    "xgboost_model_historical, xgb_Y_pred_historical, xgb_mae_historical, xgb_Y_pred_prob_historical = Model.xgboost(df_test_historical_encode_train, df_test_historical_encode_test, xgb_params, 1)\n",
    "print(\"XGB MAE\",xgb_mae_historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\tlearn: 0.6701213\ttest: 0.6719566\tbest: 0.6719566 (0)\ttotal: 85.9ms\tremaining: 8.5s\n",
      "1:\tlearn: 0.6550407\ttest: 0.6595996\tbest: 0.6595996 (1)\ttotal: 163ms\tremaining: 7.98s\n",
      "2:\tlearn: 0.6438808\ttest: 0.6509606\tbest: 0.6509606 (2)\ttotal: 249ms\tremaining: 8.05s\n",
      "3:\tlearn: 0.6352730\ttest: 0.6441494\tbest: 0.6441494 (3)\ttotal: 332ms\tremaining: 7.96s\n",
      "4:\tlearn: 0.6289268\ttest: 0.6392159\tbest: 0.6392159 (4)\ttotal: 415ms\tremaining: 7.89s\n",
      "5:\tlearn: 0.6240797\ttest: 0.6357934\tbest: 0.6357934 (5)\ttotal: 500ms\tremaining: 7.83s\n",
      "6:\tlearn: 0.6206361\ttest: 0.6336700\tbest: 0.6336700 (6)\ttotal: 597ms\tremaining: 7.92s\n",
      "7:\tlearn: 0.6158242\ttest: 0.6296205\tbest: 0.6296205 (7)\ttotal: 687ms\tremaining: 7.9s\n",
      "8:\tlearn: 0.6131403\ttest: 0.6280576\tbest: 0.6280576 (8)\ttotal: 762ms\tremaining: 7.7s\n",
      "9:\tlearn: 0.6111912\ttest: 0.6271252\tbest: 0.6271252 (9)\ttotal: 858ms\tremaining: 7.72s\n",
      "10:\tlearn: 0.6082986\ttest: 0.6246954\tbest: 0.6246954 (10)\ttotal: 941ms\tremaining: 7.61s\n",
      "11:\tlearn: 0.6065897\ttest: 0.6234829\tbest: 0.6234829 (11)\ttotal: 1.02s\tremaining: 7.51s\n",
      "12:\tlearn: 0.6050371\ttest: 0.6226185\tbest: 0.6226185 (12)\ttotal: 1.12s\tremaining: 7.5s\n",
      "13:\tlearn: 0.6030165\ttest: 0.6207445\tbest: 0.6207445 (13)\ttotal: 1.2s\tremaining: 7.38s\n",
      "14:\tlearn: 0.6008854\ttest: 0.6182263\tbest: 0.6182263 (14)\ttotal: 1.29s\tremaining: 7.29s\n",
      "15:\tlearn: 0.5997166\ttest: 0.6177654\tbest: 0.6177654 (15)\ttotal: 1.39s\tremaining: 7.29s\n",
      "16:\tlearn: 0.5986491\ttest: 0.6172899\tbest: 0.6172899 (16)\ttotal: 1.47s\tremaining: 7.19s\n",
      "17:\tlearn: 0.5977728\ttest: 0.6169012\tbest: 0.6169012 (17)\ttotal: 1.55s\tremaining: 7.07s\n",
      "18:\tlearn: 0.5953042\ttest: 0.6152200\tbest: 0.6152200 (18)\ttotal: 1.66s\tremaining: 7.06s\n",
      "19:\tlearn: 0.5942008\ttest: 0.6148644\tbest: 0.6148644 (19)\ttotal: 1.74s\tremaining: 6.97s\n",
      "20:\tlearn: 0.5934722\ttest: 0.6145958\tbest: 0.6145958 (20)\ttotal: 1.82s\tremaining: 6.85s\n",
      "21:\tlearn: 0.5926608\ttest: 0.6144494\tbest: 0.6144494 (21)\ttotal: 1.95s\tremaining: 6.91s\n",
      "22:\tlearn: 0.5916291\ttest: 0.6141175\tbest: 0.6141175 (22)\ttotal: 2.07s\tremaining: 6.92s\n",
      "23:\tlearn: 0.5907353\ttest: 0.6138140\tbest: 0.6138140 (23)\ttotal: 2.17s\tremaining: 6.88s\n",
      "24:\tlearn: 0.5899452\ttest: 0.6134966\tbest: 0.6134966 (24)\ttotal: 2.26s\tremaining: 6.78s\n",
      "25:\tlearn: 0.5891428\ttest: 0.6131344\tbest: 0.6131344 (25)\ttotal: 2.34s\tremaining: 6.67s\n",
      "26:\tlearn: 0.5886409\ttest: 0.6130212\tbest: 0.6130212 (26)\ttotal: 2.43s\tremaining: 6.58s\n",
      "27:\tlearn: 0.5873379\ttest: 0.6119897\tbest: 0.6119897 (27)\ttotal: 2.52s\tremaining: 6.47s\n",
      "28:\tlearn: 0.5864530\ttest: 0.6114085\tbest: 0.6114085 (28)\ttotal: 2.6s\tremaining: 6.37s\n",
      "29:\tlearn: 0.5854411\ttest: 0.6112864\tbest: 0.6112864 (29)\ttotal: 2.71s\tremaining: 6.31s\n",
      "30:\tlearn: 0.5849819\ttest: 0.6111917\tbest: 0.6111917 (30)\ttotal: 2.79s\tremaining: 6.2s\n",
      "31:\tlearn: 0.5844415\ttest: 0.6110936\tbest: 0.6110936 (31)\ttotal: 2.87s\tremaining: 6.09s\n",
      "32:\tlearn: 0.5837481\ttest: 0.6109707\tbest: 0.6109707 (32)\ttotal: 2.97s\tremaining: 6.03s\n",
      "33:\tlearn: 0.5831289\ttest: 0.6107519\tbest: 0.6107519 (33)\ttotal: 3.05s\tremaining: 5.92s\n",
      "34:\tlearn: 0.5817683\ttest: 0.6102049\tbest: 0.6102049 (34)\ttotal: 3.13s\tremaining: 5.82s\n",
      "35:\tlearn: 0.5808335\ttest: 0.6098449\tbest: 0.6098449 (35)\ttotal: 3.27s\tremaining: 5.81s\n",
      "36:\tlearn: 0.5802406\ttest: 0.6097789\tbest: 0.6097789 (36)\ttotal: 3.38s\tremaining: 5.75s\n",
      "37:\tlearn: 0.5795008\ttest: 0.6095871\tbest: 0.6095871 (37)\ttotal: 3.5s\tremaining: 5.71s\n",
      "38:\tlearn: 0.5787784\ttest: 0.6094977\tbest: 0.6094977 (38)\ttotal: 3.6s\tremaining: 5.64s\n",
      "39:\tlearn: 0.5782829\ttest: 0.6093594\tbest: 0.6093594 (39)\ttotal: 3.72s\tremaining: 5.58s\n",
      "40:\tlearn: 0.5779758\ttest: 0.6092615\tbest: 0.6092615 (40)\ttotal: 3.84s\tremaining: 5.52s\n",
      "41:\tlearn: 0.5772946\ttest: 0.6090419\tbest: 0.6090419 (41)\ttotal: 3.95s\tremaining: 5.45s\n",
      "42:\tlearn: 0.5768884\ttest: 0.6090506\tbest: 0.6090419 (41)\ttotal: 4.07s\tremaining: 5.4s\n",
      "43:\tlearn: 0.5763502\ttest: 0.6090125\tbest: 0.6090125 (43)\ttotal: 4.17s\tremaining: 5.31s\n",
      "44:\tlearn: 0.5758302\ttest: 0.6089789\tbest: 0.6089789 (44)\ttotal: 4.28s\tremaining: 5.23s\n",
      "45:\tlearn: 0.5755267\ttest: 0.6089750\tbest: 0.6089750 (45)\ttotal: 4.36s\tremaining: 5.12s\n",
      "46:\tlearn: 0.5746912\ttest: 0.6084319\tbest: 0.6084319 (46)\ttotal: 4.48s\tremaining: 5.05s\n",
      "47:\tlearn: 0.5737910\ttest: 0.6080726\tbest: 0.6080726 (47)\ttotal: 4.59s\tremaining: 4.98s\n",
      "48:\tlearn: 0.5729636\ttest: 0.6081155\tbest: 0.6080726 (47)\ttotal: 4.69s\tremaining: 4.88s\n",
      "49:\tlearn: 0.5724248\ttest: 0.6081336\tbest: 0.6080726 (47)\ttotal: 4.78s\tremaining: 4.78s\n",
      "50:\tlearn: 0.5720136\ttest: 0.6080999\tbest: 0.6080726 (47)\ttotal: 4.85s\tremaining: 4.66s\n",
      "51:\tlearn: 0.5716583\ttest: 0.6080822\tbest: 0.6080726 (47)\ttotal: 4.93s\tremaining: 4.55s\n",
      "52:\tlearn: 0.5708228\ttest: 0.6080773\tbest: 0.6080726 (47)\ttotal: 5.03s\tremaining: 4.46s\n",
      "53:\tlearn: 0.5704787\ttest: 0.6080668\tbest: 0.6080668 (53)\ttotal: 5.11s\tremaining: 4.35s\n",
      "54:\tlearn: 0.5694301\ttest: 0.6081140\tbest: 0.6080668 (53)\ttotal: 5.21s\tremaining: 4.26s\n",
      "55:\tlearn: 0.5688388\ttest: 0.6079084\tbest: 0.6079084 (55)\ttotal: 5.28s\tremaining: 4.15s\n",
      "56:\tlearn: 0.5683547\ttest: 0.6076601\tbest: 0.6076601 (56)\ttotal: 5.37s\tremaining: 4.05s\n",
      "57:\tlearn: 0.5678315\ttest: 0.6076888\tbest: 0.6076601 (56)\ttotal: 5.45s\tremaining: 3.95s\n",
      "58:\tlearn: 0.5672085\ttest: 0.6075213\tbest: 0.6075213 (58)\ttotal: 5.54s\tremaining: 3.85s\n",
      "59:\tlearn: 0.5664176\ttest: 0.6074002\tbest: 0.6074002 (59)\ttotal: 5.63s\tremaining: 3.75s\n",
      "60:\tlearn: 0.5661456\ttest: 0.6073429\tbest: 0.6073429 (60)\ttotal: 5.71s\tremaining: 3.65s\n",
      "61:\tlearn: 0.5655671\ttest: 0.6075225\tbest: 0.6073429 (60)\ttotal: 5.8s\tremaining: 3.55s\n",
      "62:\tlearn: 0.5650556\ttest: 0.6074747\tbest: 0.6073429 (60)\ttotal: 5.88s\tremaining: 3.46s\n",
      "63:\tlearn: 0.5644263\ttest: 0.6074424\tbest: 0.6073429 (60)\ttotal: 5.98s\tremaining: 3.37s\n",
      "64:\tlearn: 0.5636937\ttest: 0.6073030\tbest: 0.6073030 (64)\ttotal: 6.06s\tremaining: 3.26s\n",
      "65:\tlearn: 0.5632900\ttest: 0.6072875\tbest: 0.6072875 (65)\ttotal: 6.14s\tremaining: 3.16s\n",
      "66:\tlearn: 0.5626780\ttest: 0.6072127\tbest: 0.6072127 (66)\ttotal: 6.24s\tremaining: 3.07s\n",
      "67:\tlearn: 0.5621789\ttest: 0.6071183\tbest: 0.6071183 (67)\ttotal: 6.32s\tremaining: 2.97s\n",
      "68:\tlearn: 0.5618764\ttest: 0.6070782\tbest: 0.6070782 (68)\ttotal: 6.41s\tremaining: 2.88s\n",
      "69:\tlearn: 0.5614419\ttest: 0.6070443\tbest: 0.6070443 (69)\ttotal: 6.5s\tremaining: 2.78s\n",
      "70:\tlearn: 0.5611182\ttest: 0.6070320\tbest: 0.6070320 (70)\ttotal: 6.58s\tremaining: 2.69s\n",
      "71:\tlearn: 0.5605579\ttest: 0.6070425\tbest: 0.6070320 (70)\ttotal: 6.66s\tremaining: 2.59s\n",
      "72:\tlearn: 0.5600406\ttest: 0.6070718\tbest: 0.6070320 (70)\ttotal: 6.75s\tremaining: 2.5s\n",
      "73:\tlearn: 0.5596794\ttest: 0.6069961\tbest: 0.6069961 (73)\ttotal: 6.83s\tremaining: 2.4s\n",
      "74:\tlearn: 0.5593432\ttest: 0.6069989\tbest: 0.6069961 (73)\ttotal: 6.91s\tremaining: 2.3s\n",
      "75:\tlearn: 0.5589328\ttest: 0.6069930\tbest: 0.6069930 (75)\ttotal: 7s\tremaining: 2.21s\n",
      "76:\tlearn: 0.5584018\ttest: 0.6070179\tbest: 0.6069930 (75)\ttotal: 7.09s\tremaining: 2.12s\n",
      "77:\tlearn: 0.5579422\ttest: 0.6070323\tbest: 0.6069930 (75)\ttotal: 7.17s\tremaining: 2.02s\n",
      "78:\tlearn: 0.5573190\ttest: 0.6069961\tbest: 0.6069930 (75)\ttotal: 7.27s\tremaining: 1.93s\n",
      "79:\tlearn: 0.5565432\ttest: 0.6070335\tbest: 0.6069930 (75)\ttotal: 7.35s\tremaining: 1.84s\n",
      "80:\tlearn: 0.5562518\ttest: 0.6069443\tbest: 0.6069443 (80)\ttotal: 7.42s\tremaining: 1.74s\n",
      "81:\tlearn: 0.5554428\ttest: 0.6069263\tbest: 0.6069263 (81)\ttotal: 7.52s\tremaining: 1.65s\n",
      "82:\tlearn: 0.5548834\ttest: 0.6069177\tbest: 0.6069177 (82)\ttotal: 7.61s\tremaining: 1.56s\n",
      "83:\tlearn: 0.5541606\ttest: 0.6067860\tbest: 0.6067860 (83)\ttotal: 7.7s\tremaining: 1.47s\n",
      "84:\tlearn: 0.5537036\ttest: 0.6068018\tbest: 0.6067860 (83)\ttotal: 7.79s\tremaining: 1.38s\n",
      "85:\tlearn: 0.5531838\ttest: 0.6067721\tbest: 0.6067721 (85)\ttotal: 7.87s\tremaining: 1.28s\n",
      "86:\tlearn: 0.5526110\ttest: 0.6068130\tbest: 0.6067721 (85)\ttotal: 7.95s\tremaining: 1.19s\n",
      "87:\tlearn: 0.5521837\ttest: 0.6067393\tbest: 0.6067393 (87)\ttotal: 8.04s\tremaining: 1.1s\n",
      "88:\tlearn: 0.5515136\ttest: 0.6066394\tbest: 0.6066394 (88)\ttotal: 8.13s\tremaining: 1s\n",
      "89:\tlearn: 0.5510093\ttest: 0.6064265\tbest: 0.6064265 (89)\ttotal: 8.21s\tremaining: 912ms\n",
      "90:\tlearn: 0.5503460\ttest: 0.6064748\tbest: 0.6064265 (89)\ttotal: 8.3s\tremaining: 821ms\n",
      "91:\tlearn: 0.5496711\ttest: 0.6065229\tbest: 0.6064265 (89)\ttotal: 8.39s\tremaining: 729ms\n",
      "92:\tlearn: 0.5491350\ttest: 0.6065892\tbest: 0.6064265 (89)\ttotal: 8.47s\tremaining: 637ms\n",
      "93:\tlearn: 0.5487136\ttest: 0.6064895\tbest: 0.6064265 (89)\ttotal: 8.56s\tremaining: 546ms\n",
      "94:\tlearn: 0.5480646\ttest: 0.6064310\tbest: 0.6064265 (89)\ttotal: 8.64s\tremaining: 455ms\n",
      "95:\tlearn: 0.5477458\ttest: 0.6064318\tbest: 0.6064265 (89)\ttotal: 8.72s\tremaining: 363ms\n",
      "96:\tlearn: 0.5471357\ttest: 0.6064400\tbest: 0.6064265 (89)\ttotal: 8.81s\tremaining: 272ms\n",
      "97:\tlearn: 0.5466248\ttest: 0.6064314\tbest: 0.6064265 (89)\ttotal: 8.9s\tremaining: 182ms\n",
      "98:\tlearn: 0.5461148\ttest: 0.6065144\tbest: 0.6064265 (89)\ttotal: 8.98s\tremaining: 90.7ms\n",
      "99:\tlearn: 0.5456743\ttest: 0.6064961\tbest: 0.6064265 (89)\ttotal: 9.07s\tremaining: 0us\n",
      "\n",
      "bestTest = 0.6064265025\n",
      "bestIteration = 89\n",
      "\n",
      "Shrink model to first 90 iterations.\n",
      "Catboost MAE 0.33136145650862936\n"
     ]
    }
   ],
   "source": [
    "catboost_params = { \"depth\":9, \"random_seed\":42, \"iterations\":100, \"loss_function\":\"Logloss\", \"verbose\":1,\"learning_rate\":0.15}\n",
    "catboost_model_val_historical, catboost_Y_pred_val_historical, catboost_mae_val_historical, catboost_Y_pred_prob_val_historical = Model.catboost(df_test_historical_encode_train, df_test_historical_encode_test, catboost_params, 1)\n",
    "print(\"Catboost MAE\",catboost_mae_val_historical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('item_price_item_id_max', 0.059217863),\n",
       " ('delivery_weekday', 0.05522489),\n",
       " ('order_item_id_size_nunique', 0.053468738),\n",
       " ('user_id', 0.05205973),\n",
       " ('item_price_item_id_mean', 0.049567867),\n",
       " ('item_price', 0.04147808),\n",
       " ('item_price_item_id_min', 0.034959912),\n",
       " ('order_sum', 0.023799375),\n",
       " ('order_number_same_item_id_item_color', 0.021620832),\n",
       " ('delivery_time', 0.019792031),\n",
       " ('brand_id', 0.01800419),\n",
       " ('item_id', 0.015281133),\n",
       " ('order_number_same_item_color_brand_id', 0.012338433),\n",
       " ('order_brand_id_size_nunique', 0.012132957),\n",
       " ('order_number_different_size_brand_id', 0.012038885),\n",
       " ('delivery_week', 0.011983824),\n",
       " ('order_item_count', 0.011448588),\n",
       " ('item_price_brand_id_median', 0.010690354),\n",
       " ('order_number_different_item_id_size_item_color', 0.010572391),\n",
       " ('order_number_different_size_item_color_brand_id', 0.010266334),\n",
       " ('item_price_user_id_sum', 0.00991724),\n",
       " ('order_item_id_nunique', 0.009706765),\n",
       " ('item_price_user_id_count', 0.0090891225),\n",
       " ('order_number_different_brand_id', 0.008615333),\n",
       " ('order_item_id_color_nunique', 0.008426704),\n",
       " ('order_number_same_item_id', 0.0082380995),\n",
       " ('mode_brand_id', 0.008145884),\n",
       " ('order_number_different_item_id_size', 0.007966904),\n",
       " ('item_price_item_id_mad', 0.0078867795),\n",
       " ('price-item_price_brand_id_max', 0.0077574146),\n",
       " ('order_number_same_brand_id', 0.007689506),\n",
       " ('item_price_item_id_median', 0.007658195),\n",
       " ('item_price_brand_id_mad', 0.0076139015),\n",
       " ('price-item_price_user_id_max', 0.007534534),\n",
       " ('order_item_color_nunique', 0.0075288396),\n",
       " ('order_brand_id_color_nunique', 0.007471192),\n",
       " ('order_number_same_item_id_size', 0.007379356),\n",
       " ('order_number_same_size', 0.007350799),\n",
       " ('item_price_user_id_std', 0.0072924993),\n",
       " ('price-item_price_brand_id_min', 0.0072792536),\n",
       " ('item_price_user_id_mad', 0.007161255),\n",
       " ('order_brand_id_nunique', 0.0071444144),\n",
       " ('user_age', 0.0071257087),\n",
       " ('item_price_item_id_std', 0.0070624915),\n",
       " ('item_price_brand_id_max', 0.007057715),\n",
       " ('item_price_brand_id_mean', 0.0070460974),\n",
       " ('user_title', 0.0070184027),\n",
       " ('item_price_user_id_max', 0.0070025683),\n",
       " ('item_price_item_id_count', 0.0069753476),\n",
       " ('item_price_brand_id_count', 0.0069207377),\n",
       " ('item_price_user_id_median', 0.006873685),\n",
       " ('item_price_brand_id_min', 0.006829922),\n",
       " ('order_number_different_item_color_brand_id', 0.0068102926),\n",
       " ('mode_item_id', 0.006779661),\n",
       " ('item_price_brand_id_sum', 0.0067453487),\n",
       " ('user_reg_age', 0.006717793),\n",
       " ('item_price_item_id_sum', 0.0066105304),\n",
       " ('order_number_same_size_brand_id', 0.006566228),\n",
       " ('price-item_price_item_id_max', 0.006426825),\n",
       " ('mode_size', 0.0064145923),\n",
       " ('item_price_brand_id_std', 0.006407458),\n",
       " ('order_number_same_size_item_color', 0.0063752835),\n",
       " ('price-item_price_item_id_min', 0.006373869),\n",
       " ('order_brand_id_item_id_nunique', 0.0063492074),\n",
       " ('order_number_different_item_id_item_color', 0.0063242973),\n",
       " ('order_weekday', 0.006126216),\n",
       " ('price-item_price_brand_id_mean', 0.00610939),\n",
       " ('item_price_user_id_mean', 0.006071371),\n",
       " ('price-item_price_item_id_mean', 0.006071341),\n",
       " ('average_item_price_order', 0.0060559795),\n",
       " ('mode_item_color', 0.0060048257),\n",
       " ('delivery_month', 0.0058194553),\n",
       " ('price-item_price_user_id_mean', 0.005707789),\n",
       " ('user_state', 0.0056306827),\n",
       " ('item_price_user_id_min', 0.0055765426),\n",
       " ('price-item_price_user_id_min', 0.0055700666),\n",
       " ('delivery_day', 0.005515271),\n",
       " ('order_day', 0.0055129784),\n",
       " ('order_number_different_size_item_color', 0.0053966576),\n",
       " ('size', 0.0053936657),\n",
       " ('order_number_different_item_id', 0.0053829923),\n",
       " ('order_size_nunique', 0.00536293),\n",
       " ('order_week', 0.005207387),\n",
       " ('order_number_different_item_color', 0.0051620333),\n",
       " ('order_number_different_size', 0.0051172976),\n",
       " ('item_color', 0.004610815),\n",
       " ('order_month', 0.0045510083),\n",
       " ('order_number_same_size_item_color_brand_id', 0.0044929073),\n",
       " ('order_number_same_item_color', 0.0043991175),\n",
       " ('order_number_same_item_id_size_item_color', 0.0035369187)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sorted(zip(df_test_historical_encode_train.columns.drop(\"return\"), xgboost_model_historical.feature_importances_), key=lambda xx: xx[1], reverse=True))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fd16a1c2e981052eaae61151b9525ae9913f1f0d16bca6b7e7be9e0f29d739d2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
